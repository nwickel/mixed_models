[["index.html", "Course on Mixed-effects Models Course information Desirable background knowledge Overview of dates and topics Literature", " Course on Mixed-effects Models Nora Wickelmaier 2025-06-16 Course information This is an online book accompanying a course on mixed-effects models taught to PhD students at the Leibniz-Institut für Wissensmedien (IWM) in Tübingen. The data sets used in the examples and exercises can be downloaded here. Desirable background knowledge Alle examples in this course are worked out in R. Introductory knowledge in R is therefore a prerequisite. Apart from this, I will assume that you have some kind of workflow how to use R with a text editor (e.g., Vim) or an IDE (e.g., RStudio). It is further assumed that you have attended at least one introductory statistics course (level “Statisik I” and “Statistik II” usually taught in psychology curricula at German universities). That means you should be familiar with concepts like random variables, statistical distributions like the normal distribution, \\(t\\) distribution, Binomial distribution, etc., and hypothesis testing. Some background in regression analysis is helpful, but we will cover the basics in this course. If you feel like these concepts could be a bit more “present” in your head, you can go through the first chapter in the online book by Vasishth et al. (2022). There will be time to ask questions about the concepts in the sessions, but we will not have time to go through all of them together. Hopefully, these resources will help to get everybody on the same page. If you need more support or materials, just let me know. Overview of dates and topics WS 2024/25 Chapter Date Topic 1 28.10.2024 Simple and multiple regression 2 11.11.2024 Generalized linear models 3 25.11.2024 Pre/post measurements 4 09.12.2024 Longitudinal data 5 13.01.2025 Repeated measures 6 27.01.2025 Growth curve models 7 10.02.2025 Crossed random effects SS 2025 Chapter Date Topic 8 14.04.2025 Random effects for within-subject designs 9 28.04.2025 Contrast coding 10 12.05.2025 Multilevel models 11 26.05.2025 Generalized linear mixed-effects models 12 23.06.2025 Data simulation in R 13 07.07.2025 Power simulation longitudinal data 14 21.07.2025 Power simulation (G)LMM within design Literature This course is mostly built on the following books and papers: Alday et al. (2025) Baayen et al. (2008) Bates et al. (2015) Gelman et al. (2020) Vasishth et al. (2022) Wickelmaier (2022) References Alday, P., Kliegl, R. &amp; Bates, D. (2025). Embrace uncertainty – Mixed-effects models with Julia. https://embraceuncertaintybook.com/ Baayen, R. H., Davidson, D. J. &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005 Bates, D., Mächler, M., Bolker, B. &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01 Gelman, A., Hill, J. &amp; Vehtari, A. (2020). Regression and other stories. Cambridge University Press. Vasishth, S., Schad, D., Bürki, A. &amp; Kliegl, R. (2022). Linear mixed models in linguistics and psychology: A comprehensive introduction. https://vasishth.github.io/Freq_CogSci Wickelmaier, F. (2022). Simulating the power of statistical tests: A collection of R examples. ArXiv. https://arxiv.org/abs/2110.09836 "],["simple-and-multiple-regression.html", "Chapter 1 Simple and multiple regression 1.1 Slides 1.2 Example to illustrate assumptions 1.3 Exercises", " Chapter 1 Simple and multiple regression This chapter is meant to give a short introduction to regression analysis. It is mostly meant to reintroduce some basic concepts, introduce notation, and get everybody on the same page. 1.1 Slides Unable to display PDF file. Download instead. 1.2 Example to illustrate assumptions Code data(anscombe) lm1 &lt;- lm(y1 ~ x1, anscombe) lm2 &lt;- lm(y2 ~ x2, anscombe) lm3 &lt;- lm(y3 ~ x3, anscombe) lm4 &lt;- lm(y4 ~ x4, anscombe) # Compare estimates rbind(coef(lm1), coef(lm2), coef(lm3), coef(lm4)) ## (Intercept) x1 ## [1,] 3.000091 0.5000909 ## [2,] 3.000909 0.5000000 ## [3,] 3.002455 0.4997273 ## [4,] 3.001727 0.4999091 Code # Plot data par(mfrow = c(2,2), mai = c(.6, .6, .1, .1), mgp = c(2.4, 1, 0)) plot(y1 ~ x1, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm1, lwd = 2) plot(y2 ~ x2, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm2, lwd = 2) plot(y3 ~ x3, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm3, lwd = 2) plot(y4 ~ x4, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm4, lwd = 2) Code # Look at assumption violations par(mfrow = c(2,2)) plot(lm1) Code plot(lm2) Code plot(lm3) Code plot(lm4) Code # Reshape data frame to long format dat &lt;- reshape(anscombe, direction = &quot;long&quot;, varying = list(paste0(&quot;x&quot;, 1:4), paste0(&quot;y&quot;, 1:4)), timevar = &quot;reg&quot;, v.names = c(&quot;x&quot;, &quot;y&quot;))[, -4] # drop id lattice::xyplot(y ~ x | as.factor(reg), dat, pch = 16, type = c(&quot;p&quot;, &quot;r&quot;)) Code lattice::xyplot(y ~ x | as.factor(reg), dat, pch = 16, type = c(&quot;p&quot;, &quot;smooth&quot;)) 1.3 Exercises Exercise 1 Simulate a data set based on a simple regression model with \\[\\begin{align*} \\beta_0 &amp; = 0.2\\\\ \\beta_1 &amp; = 0.3\\\\ \\sigma &amp; = 0.5\\\\ x &amp; \\in [1, 20]~\\text{in steps of 1} \\end{align*}\\] What functions in R do we need? Code x &lt;- 1:20 n &lt;- length(x) a &lt;- 0.2 b &lt;- 0.3 sigma &lt;- 0.5 y &lt;- 0.2 + 0.3*x + rnorm(n, sd = sigma) dat &lt;- data.frame(x, y) lm1 &lt;- lm(y ~ x, dat) summary(lm1) mean(resid(lm1)) sd(resid(lm1)) hist(resid(lm1), breaks = 15) Code # plot data plot(y ~ x, dat) abline(lm1) Exercise 2 Simulate data with the parameters from Exercise 1 Do not assume that we have one subject per value for \\(x\\), but more than one subject Simulate data for \\(n = 40\\) and \\(n = 100\\) Hint: Use sample(x, n, replace = TRUE) Re-cover your parameters as done on slide 14 What happens to your standard errors? Code n &lt;- 100 # 40 x0 &lt;- 1:20 x &lt;- sample(x0, n, replace = TRUE) a &lt;- 0.2 b &lt;- 0.3 sigma &lt;- 0.5 y &lt;- 0.2 + 0.3*x + rnorm(n, sd = sigma) dat &lt;- data.frame(x, y) pars &lt;- replicate(2000, { ysim &lt;- 0.2 + 0.3 * x + rnorm(n, sd = sigma) lm1 &lt;- lm(ysim ~ x, data = dat) c(coef(lm1), sigma(lm1)) }) rowMeans(pars) # standard errors apply(pars, 1, sd) hist(pars[1, ]) Code hist(pars[2, ]) Code hist(pars[3, ]) Code plot(y ~ jitter(x), dat) Exercise 3 Create two vectors \\(x\\) and \\(y\\) with 100 observations each and \\(X \\sim N(1,1)\\) and \\(Y \\sim N(2,1)\\). Create a data frame with variables id, group and score. \\(x\\) and \\(y\\) are your score values. Conduct a \\(t\\) test assuming that \\(X\\) and \\(Y\\) are independent having the same variances. Then use the function aov() to compute an analysis of variance for these data. Use then function lm() for a linear regression with predictor group and dependent variable score. Compare your results. Code x &lt;- rnorm(100, mean = 1) y &lt;- rnorm(100, mean = 2) dat &lt;- data.frame(id = 1:200, group = rep(c(&quot;x&quot;,&quot;y&quot;), each = 100), score = c(x, y)) rm(x,y) t1 &lt;- t.test(score ~ group, data = dat, var.equal = TRUE) lm1 &lt;- lm(score ~ group, data = dat) aov1 &lt;- aov(score ~ group, data = dat) (stat &lt;- list( coef = matrix(c(t1$estimate, lm1$coef, aov1$coef), nrow = 2, ncol = 3, dimnames = list(NULL, c(&quot;ttest&quot;, &quot;lm&quot;, &quot;aov&quot;))), statistics = matrix(c(t = t1$statistic^2, Flm = summary(lm1)$fstatistic[1], Faov = unlist(summary(aov1))[7]), nrow = 1, ncol = 3, dimnames = list(NULL, c(&quot;t&quot;,&quot;Flm&quot;,&quot;Faov&quot;)))) ) Exercise 4 The data set cars contains speed and stopping distances of 50 cars Estimate the regression model \\[ dist_i = \\beta_0 + \\beta_1 speed_i + \\varepsilon_i \\] How much variance of the stopping distances is explained by speed? Look at the residuals of the model. Are there any systematic deviances? Now estimate the model \\[ dist_i = \\beta_0 + \\beta_1 speed_i + \\beta_2 speed^2_i + \\varepsilon_i \\] Hint: Use I(speed^2) in the model formula Which model fits the data better? Code data(cars) lm1 &lt;- lm(dist ~ speed, data = cars) summary(lm1) hist(resid(lm1)) Code par(mfrow = c(2,2)) plot(lm1) Code lm2 &lt;- lm(dist ~ speed + I(speed^2), data = cars) anova(lm1, lm2) "],["generalized-linear-models.html", "Chapter 2 Generalized linear models 2.1 Slides 2.2 Exercises", " Chapter 2 Generalized linear models This chapter gives a hands-on introduction to generalized linear models with a focus on logistic regression for binary response data and poisson regression for count data. 2.1 Slides Unable to display PDF file. Download instead. 2.2 Exercises Exercise 1 In a psychophysical experiment two LEDs are presented to a subject: a standard with 40 cd/m\\(^2\\) and a comparison with varying intensities The subject is supposed to say which stimulus is brighter; each comparison is presented 40 times x (cd/m\\(^2\\)) 37 38 39 40 41 42 43 y (positiv) 2 3 10 25 34 36 39 Estimate parameters \\(c\\) and \\(a\\) of the logistic psychometric function \\[ p_{pos} = \\frac{1}{1 + \\exp(-\\frac{\\displaystyle x - c}{\\displaystyle a})} \\] using glm() with \\(logit(p_{pos}) = \\beta_0 + \\beta_1x\\) where \\(a = 1/\\beta_1\\) and \\(c = -\\beta_0/\\beta_1\\). Calculate the intensity \\(x\\) for which \\(p_{pos} = 0.5\\) (Point of Subjective Equality, PSE) Create a plot for the probability to give a positive answer depending on the intensity of the comparison Use predict() to obtain the predicted values and add the logistic psychometric function to the plot Use abline() to add parameter \\(c\\) to the plot Use a likelihood ratio test to assess how well the model fits the data Is there reason to assume that there is any overdispersion? Code dat &lt;- data.frame(x = 37:43, y = c(2, 3, 10, 25, 34, 36, 39), n = 40) glm1 &lt;- glm(cbind(y, n - y) ~ x, family = binomial, data = dat) a &lt;- 1 / coef(glm1)[2] c &lt;- -coef(glm1)[1] / coef(glm1)[2] newx &lt;- seq(37, 43, .1) pre &lt;- predict(glm1, newdata = data.frame(x = newx), type = &quot;response&quot;) # Plot predictions with PSE plot(y/n ~ x, data = dat, pch = 16, ylab = &quot;Probability to say brighter&quot;) lines(pre ~ newx, data = dat) abline(v = c, h = .5, lty = 3) text(39, .8, paste(&quot;PSE =&quot;, round(c, 2))) Code # Goodness-of-fit test glms &lt;- glm(cbind(y, n - y) ~ factor(x), family = binomial, data = dat) anova(glm1, glms, test = &quot;Chisq&quot;) # Overdispersion summary(glm(cbind(y, n - y) ~ x, family = quasibinomial, data = dat)) Exercise 2 Fit a regression model to the Affairs data set from the AER package in R The variable affairs is the number of extramarital affairs in the past year and is our response variable Include the variables gender, age, yearsmarried, children, religiousness, education and rating as predictors religiousness ranges from 1 (anti) to 5 (very) and rating is a self rating of the marriage, ranging from 1 (very unhappy) to 5 (very happy) Assess the Goodness-of-fit using the deviance Assess overdispersion and decide if a model with an extra dispersion parameter might be indicated Compare the confidence intervals for the estimated parameters for both models Code # Load data set data(Affairs, package = &quot;AER&quot;) Affairs$rating &lt;- factor(Affairs$rating, levels = 1:5) ## Fit poisson model pois1 &lt;- glm(affairs ~ gender + age + yearsmarried + children + education + rating, family = poisson, data = Affairs) summary(pois1) # Goodness-of-fit 1 - pchisq(pois1$deviance, df = pois1$df.residual) ## Model with dispersion parameter pois2 &lt;- glm(affairs ~ gender + age + yearsmarried + children + education + rating, family = quasipoisson, data = Affairs) summary(pois2) ## Compare CIs confint(pois1) confint(pois2) Visualize model predictions Code ## Plot predictions for &quot;average&quot; females newdat &lt;- expand.grid(yearsmarried = seq(0, 15, 1), rating = factor(1:5)) newdat$gender &lt;- &quot;female&quot; newdat$age &lt;- mean(Affairs$age) newdat$children &lt;- &quot;yes&quot; newdat$religiousness &lt;- mean(Affairs$religiousness) newdat$education &lt;- mean(Affairs$education) newdat$pre &lt;- predict(pois2, newdata = newdat, type = &quot;response&quot;) colors &lt;- c(&quot;#78004B&quot;, &quot;#3CB4DC&quot;, &quot;#91C86E&quot;, &quot;#FF6900&quot;, &quot;#434F4F&quot;) plot(pre ~ yearsmarried, newdat, type = &quot;n&quot;, main = &quot;&#39;Average&#39; Females&quot;) for (i in 1:5) { lines(pre ~ yearsmarried, newdat[newdat$rating == i, ], col = colors[i]) } legend(&quot;topleft&quot;, c(&quot;very unhappy&quot;, &quot;somewhat unhappy&quot;, &quot;average&quot;, &quot;happier than average&quot;, &quot;very happy&quot;), col = colors, lty = 1, bty = &quot;n&quot;) "],["prepost-measurements.html", "Chapter 3 Pre/post measurements 3.1 Slides 3.2 Exercises", " Chapter 3 Pre/post measurements In this chapter, we start with the simplest form of longitudinal data: pre/post measurements. 3.1 Slides Unable to display PDF file. Download instead. 3.2 Exercises Exercise 1 Lowry (2000, Chapter 17) describes the following example Based on values \\(Y\\) of a finals exam, \\(m = 3\\) learning methods introducing basic programming skills will be compared (\\(i = 1, 2, 3\\)) In each of the chosen schools, 12 students attending grade 5 have been randomly selected (\\(j = 1, \\dots, 12\\)) for a six week course Before the course, familiarity with computers was assessed in a pre test as covariate \\(X\\) Data for method 1: pre 14 10 7 18 14 16 13 15 5 18 16 10 post 29 24 14 27 27 28 27 32 13 35 32 17 Data for method 2: pre 6 16 9 19 13 14 15 18 17 8 15 16 post 15 28 13 36 29 27 31 33 32 15 30 26 Data for method 3: pre 15 9 7 12 12 9 12 3 13 10 11 8 post 32 27 15 23 26 17 25 14 29 22 30 25 Fit the following models to the data \\[\\begin{align} y_{ij} &amp;= \\beta_0 + \\beta_1 \\cdot x_{ij} + \\varepsilon_{ij}\\\\ y_{ij} &amp;= \\beta_0 + \\beta_1 \\cdot x_{ij} + \\beta_2 \\cdot z_{i} + \\varepsilon_{ij}\\\\ y_{ij} &amp;= \\beta_0 + \\beta_1 \\cdot x_{ij} + \\beta_2 \\cdot z_{i} + \\beta_3 \\cdot x_{ij} \\cdot z_i + \\varepsilon_{ij} \\end{align}\\] Plot the data with the predictions for each model Compare the models with a likelihood ratio test What are the adjusted means for the ANCOVA model? Code ## Fit models lm1 &lt;- lm(post ~ pre, data = dat) lm2 &lt;- lm(post ~ pre + method, data = dat) lm3 &lt;- lm(post ~ pre * method, data = dat) ## LRT anova(lm1, lm2, lm3) colors &lt;- c(&quot;#78004B&quot;, &quot;#FF6900&quot;, &quot;#3CB4DC&quot;) ## Plot model predictions par(mfrow = c(1, 3)) plot(post ~ pre, data = dat, col = colors[dat$method], pch = 16) abline(lm1) legend(&quot;topleft&quot;, paste(&quot;Method&quot;, 1:3), col = colors, pch = 16, bty = &quot;n&quot;) plot(post ~ pre, data = dat, col = colors[dat$method], pch = 16) abline(coef(lm2)[1], coef(lm2)[2], col = colors[1]) abline(coef(lm2)[1] + coef(lm2)[3], coef(lm2)[2], col = colors[2]) abline(coef(lm2)[1] + coef(lm2)[4], coef(lm2)[2], col = colors[3]) legend(&quot;topleft&quot;, paste(&quot;Method&quot;, 1:3), col = colors, pch = 16, bty = &quot;n&quot;) plot(post ~ pre, data = dat, col = colors[dat$method], pch = 16) abline(coef(lm3)[1], coef(lm3)[2], col = colors[1]) abline(coef(lm3)[1] + coef(lm3)[3], coef(lm3)[2] + coef(lm3)[5], col = colors[2]) abline(coef(lm3)[1] + coef(lm3)[4], coef(lm3)[2] + coef(lm3)[6], col = colors[3]) legend(&quot;topleft&quot;, paste(&quot;Method&quot;, 1:3), col = colors, pch = 16, bty = &quot;n&quot;) Code ## Means for pre and post measurements datagg &lt;- aggregate(cbind(pre, post) ~ method, data = dat, FUN = mean) ## Adjusted means datagg$post_adj &lt;- predict(lm2, newdata = data.frame(method = factor(1:3), pre = mean(dat$pre))) # Adjusted &quot;time effects&quot; datagg$diff &lt;- datagg$post - datagg$pre datagg$diff_adj &lt;- datagg$post_adj - mean(datagg$pre) Exercise 2 Simulate a data set for 75 subjects Create a factor condition indicating a control group and two treatment groups with 25 subjects each Simulate pre test data with \\(N \\sim (5, 1)\\) Simulate a post test score assuming a slope of 0.7 between pre and post score and that the first treatment group improves by 0.5 points compared to the control group and the second treatment group by 1 point What other assumption needs to be made? Code set.seed(1042) # set seed for reproducibility n &lt;- 75 condition &lt;- factor(rep(c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;), each = n / 3)) eff_cond &lt;- c(1, 0.5, 1) pre &lt;- rnorm(n, mean = 5, sd = 1) post &lt;- 0.7 * pre + model.matrix( ~ condition) %*% eff_cond + rnorm(n) dat &lt;- data.frame(id = factor(1:n), pre = pre, post = post, condition = condition ) rm(pre, post, condition) lattice::xyplot(post ~ pre, dat, groups = condition, type = c(&quot;g&quot;, &quot;p&quot;, &quot;r&quot;), auto.key = TRUE) Code # Observed averaged time effects, pre and post scores aggregate(cbind(post - pre, pre, post) ~ condition, dat, mean) Fit three different models to the data A Change Score model An ANCOVA model A mixed-effects model with predictors condition and time Compare the results of the three models What is the adjusted time effect for each group? Code library(lme4) ## Change Score Model #m1 &lt;- lm(post - pre ~ condition, dat) m1 &lt;- lm(post ~ condition + offset(pre), dat) summary(m1) ## ANCOVA Model m2 &lt;- lm(post ~ condition + pre, dat) summary(m2) # Adjusted means predict(m2, newdata = data.frame(condition = c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;), pre = mean(dat$pre))) ## Mixed Model (equivalent to change score model) dat_long &lt;- reshape(dat, direction = &quot;long&quot;, varying = list(c(&quot;pre&quot;, &quot;post&quot;)), v.names = &quot;resp&quot;, idvar = &quot;id&quot;, timevar = &quot;time&quot;, times = c(0, 1)) m3 &lt;- lmer(resp ~ condition * time + (1 | id), dat_long) summary(m3) ## Compare parameters of the three models cbind(coef(m1), coef(m2)[1:3], fixef(m3)[4:6]) datagg &lt;- aggregate(cbind(pre, post) ~ condition, data = dat, FUN = mean) ## Adjusted means datagg$post_adj &lt;- predict(m2, newdata = data.frame(condition = factor(c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;)), pre = mean(dat$pre))) # Adjusted &quot;time effects&quot; datagg$diff &lt;- datagg$post - datagg$pre datagg$diff_adj &lt;- datagg$post_adj - mean(datagg$pre) # OR m4 &lt;- lm(post - pre ~ condition + scale(pre), dat) summary(m4) # Adjusted &quot;time effects&quot; predict(m4, newdata = data.frame(condition = c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;), pre = mean(dat$pre))) Exercise 3 Hedeker &amp; Gibbons (2006) report a smoking prevention study with students. The study used a \\(2 \\times 2\\) factorial design with factor “prevention curriculum” (with vs. without) and “TV prevention” (with vs. without). Students were randomly assigned to one of the four groups. Students’ knowledge about tabacco induced health risks before and after the prevention have been measured. Read the data set television.txt into R. Assign meaningful variable names. (You will find the necessary information at the beginning of the data file!) Calculate means and standard deviations for baseline, follow-up and change scores separately for the four groups. How many students are assigned to each group? Plot the follow-up means. Put intervention curriculum on the \\(y\\) axis and two seperate lines for each tv condition. Which effects do you expect based on this plot? Conduct an analysis for the follow-up scores (ANOVA), the change scores, the adjusted follow-up scores (ANCOVA), and the adjusted change score. Interpret the parameters for these models. How does the knowledge of the students change depending on the two factors? Assess how well the assumptions for the ANCOVA model hold using visual methods. Code dat &lt;- read.table(&quot;data/television.txt&quot;, skip = 43) names(dat) &lt;- c(&quot;school&quot;, &quot;class&quot;, &quot;curri&quot;, &quot;tv&quot;, &quot;pre&quot;, &quot;post&quot;) aggregate(cbind(pre, post, post - pre) ~ curri + tv, data = dat, FUN = mean) aggregate(cbind(pre, post) ~ curri + tv, data = dat, FUN = sd) aggregate(pre ~ curri + tv, data = dat, FUN = length) interaction.plot(dat$curri, dat$tv, dat$post, type = &quot;b&quot;, pch = c(1,16), xlab = &quot;Curriculum&quot;, ylab = &quot;Mean THKS score&quot;, trace.label = &quot;TV&quot;) Code summary(lm1 &lt;- lm(post ~ curri * tv, data = dat)) # Follow-Up summary(lm2 &lt;- lm(post - pre ~ curri * tv, data = dat)) # Change-Score summary(lm3 &lt;- lm(post ~ pre + curri * tv, data = dat)) # ANCOVA summary(lm4 &lt;- lm(post - pre ~ pre + curri * tv, data = dat)) # Change-Score ANCOVA plot(lm3) References Hedeker, D. R. &amp; Gibbons, R. D. (2006). Longitudinal data analysis. John Wiley. Lowry, R. (2000). Concepts and applications of inferential statistics. http://vassarstats.net/textbook/ "],["longitudinal-data.html", "Chapter 4 Longitudinal data 4.1 Slides 4.2 Exercises", " Chapter 4 Longitudinal data This chapter gives an introduction to linear mixed-effects models using a standard example: the sleepstudy data set. Random intercept and random slope models are introduced. 4.1 Slides Unable to display PDF file. Download instead. 4.2 Exercises Exercise 1 Load (or maybe install first) the package languageR. Then, load the data set quasif from this package. You can use ?quasif to inspect the data set. Use the function xtabs() to inspect the structure of your data. Are your factors crossed or nested? Create a box plot looking at the distribution of reaction time for short and long SOAs Visualize your data points individually for each subject. Use xyplot() from the lattice package or functions from the package ggplot2 Fit a model with random intercepts for Item and random intercepts as well as random slopes for Subject Test your random effects using likelihood-ratio tests Decide which model fits the data (empirically) best Compute confidence intervals for the estimated parameters of this model Look at the model assumptions for this model Code library(lme4) library(lattice) # load data set data(quasif, package = &quot;languageR&quot;) boxplot(RT ~ SOA, data = quasif) Code xtabs(~ SOA + Item, data = quasif) xtabs(~ Subject + Item, data = quasif) ftable(xtabs(~ Subject + Item + SOA, data = quasif)) xyplot(RT ~ SOA | Subject, data = quasif, groups = Item, auto.key = TRUE) Code lme1 &lt;- lmer(RT ~ 1 + SOA + (1 | Item) + (1 + SOA | Subject), data = quasif, REML = FALSE) summary(lme1) lme2 &lt;- lmer(RT ~ 1 + SOA + (1 | Item) + (1 | Subject), data = quasif, REML = FALSE) lme3 &lt;- lmer(RT ~ 1 + SOA + (1 + SOA | Subject), data = quasif, REML = FALSE) lme4 &lt;- lmer(RT ~ 1 + SOA + (1 | Subject), data = quasif, REML = FALSE) anova(lme4, lme2, lme1) anova(lme4, lme3, lme1) confint(lme1, method = &quot;boot&quot;) plot(lme1, col = quasif$Subject, pch = quasif$Item) Code qqmath(lme1, col = quasif$Subject, pch = quasif$Item) "],["repeated-measurements.html", "Chapter 5 Repeated measurements 5.1 Slides 5.2 Exercises", " Chapter 5 Repeated measurements In this chapter, we will go on with models for longitudinal data and compare mixed-effects models to repeated measures ANOVA. 5.1 Slides Unable to display PDF file. Download instead. 5.2 Exercises Exercise 1 Expand the linear mixed-effects model to more than one factor Add diagnosis (“endogenous” vs. “non-endogenous”) as additional between factor Test if this factor interacts with week using a likelihood-ratio test Use parametric bootstrapping to get a sampling distribution for your LRT statistic Plot the sampling distribution and add the empirical confidence interval Plot the predictions of the model \\[ y_{ij} = \\beta_0 + \\beta_1 time + \\beta_2 diag + \\upsilon_{0i} + \\upsilon_{1i} time + \\varepsilon_{ij} \\] with \\[\\begin{align*} \\begin{pmatrix} \\upsilon_{0i}\\\\ \\upsilon_{1i} \\end{pmatrix} &amp;\\sim N \\left(\\begin{pmatrix} 0\\\\ 0 \\end{pmatrix}, \\, \\boldsymbol{\\Sigma}_\\upsilon = \\begin{pmatrix} \\sigma^2_{\\upsilon_0} &amp; \\sigma_{\\upsilon_0 \\upsilon_1} \\\\ \\sigma_{\\upsilon_0 \\upsilon_1} &amp; \\sigma^2_{\\upsilon_1} \\\\ \\end{pmatrix} \\right) \\text{ i.i.d.} \\\\ \\boldsymbol{\\varepsilon}_i &amp;\\sim N(\\mathbf{0}, \\, \\sigma^2 \\mathbf{I}_{n_i}) \\text{ i.i.d.} \\end{align*}\\] Code library(lme4) dat &lt;- read.table(&quot;data/reisby.dat&quot;, header = TRUE) dat$id &lt;- factor(dat$id) dat$diag &lt;- factor(dat$diag, levels = c(&quot;nonen&quot;, &quot;endog&quot;)) dat &lt;- na.omit(dat) # drop missing values # Add diagnosis lme1 &lt;- lmer(hamd ~ week + diag + (week | id), dat, REML = FALSE) lme2 &lt;- lmer(hamd ~ week * diag + (week | id), dat, REML = FALSE) anova(lme1, lme2) Code # Parametric bootstrapping nsim &lt;- 1000 LR &lt;- rep(NA, nsim) sim2 &lt;- simulate(lme2, nsim) for (i in seq_len(nsim)) { fit1 &lt;- lmer(sim2[, i] ~ week + diag + (week | id), dat, REML = FALSE) fit2 &lt;- lmer(sim2[, i] ~ week * diag + (week | id), dat, REML = FALSE) LR[i] &lt;- 2 * (summary(fit2)$logLik - summary(fit1)$logLik) } hist(LR, breaks = 50, col = &quot;grey&quot;, border = &quot;white&quot;, freq = FALSE, main = &quot;&quot;) curve(dchisq(x, df = 1), add = TRUE) abline(v = 2 * (summary(lme2)$logLik - summary(lme1)$logLik), lty = 2) Code # Empirical p-value mean(LR &gt; 2 * (summary(lme2)$logLik - summary(lme1)$logLik)) Code # Predictions with diagnosis as fixed effect datm &lt;- aggregate(hamd ~ week + diag, data = dat, FUN = mean) datm$pred1 &lt;- predict(lme1, newdata = datm, re.form = ~0) colors &lt;- c(&quot;#3CB4DC&quot;, &quot;#91C86E&quot;) plot(hamd ~ week, data = datm, col = colors[datm$diag], ylim = c(0, 28), xlab = &quot;Week&quot;, ylab = &quot;HDRS score&quot;) lines(pred1 ~ week, data = subset(datm, datm$diag == &quot;nonen&quot;), col = colors[1]) lines(pred1 ~ week, data = subset(datm, datm$diag == &quot;endog&quot;), col = colors[2]) legend(&quot;bottomleft&quot;, legend = c(&quot;Non endogenous&quot;, &quot;Endogenous&quot;), col = colors, pch = 21, lty = 1, pt.bg = &quot;white&quot;, bty = &quot;n&quot;) "],["growth-curve-models.html", "Chapter 6 Growth curve models 6.1 Slides 6.2 Exercises", " Chapter 6 Growth curve models In this chapter, we extend the models we have looked at so far and include quadratic effects for individuals and on the population level. 6.1 Slides Unable to display PDF file. Download instead. 6.2 Exercises Exercise 1 The following exercise is strongly inspired by this chapter: https://embraceuncertaintybook.com/longitudinal.html#the-elstongrizzle-data. Load the data set into R; data are from a dental study measuring the lengths of the ramus of the mandible (mm) in 20 boys at 8, 8.5, 9, and 9.5 years of age Plot the individual data points for each subject either as a spaghetti plot and/or as a panel plot Fit a random slope model to the data; how would you interpret the correlation parameter in the model? Recenter your time variable, so that zero means ``8 years old’’ Refit your random slope model; try to explain why and how the correlation parameter changes Look at the caterpillar plots for the random slope model with and without recentered time variable; why do they look different? Create a shrinkage plot plotting the individual intercept as a function of the individual slopes Add individual and quadratic time effects to your model; test this model against the random slope model Look at the profiles for the random effects for the quadratic model; what would you conclude? Code library(lme4) library(lattice) # Read and visualize data dat &lt;- read.table(&quot;data/elstongrizzle.dat&quot;, header = TRUE) dat$subject &lt;- factor(dat$subject) xyplot(ramusht ~ age, dat, groups = subject, type = c(&quot;b&quot;, &quot;g&quot;), xlab = &quot;Age (yr)&quot;, ylab = &quot;Ramus bone length (mm)&quot;) Code xyplot(ramusht ~ age | subject, dat, type = c(&quot;g&quot;, &quot;p&quot;, &quot;r&quot;), aspect = &quot;xy&quot;, index.cond = function(x,y) coef(lm(y ~ x)) %*% c(1, 8), xlab = &quot;Age (yr)&quot;, ylab = &quot;Ramus bone length (mm)&quot;) Code # Anything quadratic? xyplot(ramusht ~ age | subject, dat, type = c(&quot;g&quot;, &quot;p&quot;, &quot;spline&quot;), aspect = &quot;xy&quot;, index.cond = function(x,y) coef(lm(y ~ x)) %*% c(1, 8), xlab = &quot;Age (yr)&quot;, ylab = &quot;Ramus bone length (mm)&quot;) Code # Random slope model m1 &lt;- lmer(ramusht ~ age + (age | subject), dat) summary(m1) dotplot(ranef(m1), scales = list(x = list(relation = &quot;free&quot;)))[[1]] Code # Centering the time variable (at a value of interest) dat$time &lt;- dat$age - 8 m2 &lt;- lmer(ramusht ~ time + (time | subject), dat) summary(m2) dotplot(ranef(m2), scales = list(x = list(relation = &quot;free&quot;)))[[1]] Code # Shrinkage plot df &lt;- coef(lmList(ramusht ~ time | subject, dat)) fclow &lt;- subset(df, `(Intercept)` &lt; 50) fchigh &lt;- subset(df, `(Intercept)` &gt; 50) cc1 &lt;- as.data.frame(coef(m2)$subject) names(cc1) &lt;- c(&quot;A&quot;, &quot;B&quot;) df &lt;- cbind(df, cc1) ff &lt;- fixef(m2) with(df, xyplot(`(Intercept)` ~ time, aspect = 1, x1 = B, y1 = A, panel = function(x, y, x1, y1, subscripts, ...) { panel.grid(h = -1, v = -1) x1 &lt;- x1[subscripts] y1 &lt;- y1[subscripts] larrows(x, y, x1, y1, type = &quot;closed&quot;, length = 0.1, angle = 15, ...) lpoints(x, y, pch = trellis.par.get(&quot;superpose.symbol&quot;)$pch[2], col = trellis.par.get(&quot;superpose.symbol&quot;)$col[2]) lpoints(x1, y1, pch = trellis.par.get(&quot;superpose.symbol&quot;)$pch[1], col = trellis.par.get(&quot;superpose.symbol&quot;)$col[1]) lpoints(ff[2], ff[1], pch = trellis.par.get(&quot;superpose.symbol&quot;)$pch[3], col = trellis.par.get(&quot;superpose.symbol&quot;)$col[3]) ltext(fclow[,2], fclow[,1], row.names(fclow), adj = c(0.5, 1.7)) ltext(fchigh[,2], fchigh[,1], row.names(fchigh), adj = c(0.5, -0.6)) }, key = list(space = &quot;top&quot;, columns = 3, text = list(c(&quot;Mixed model&quot;, &quot;Within-subject&quot;, &quot;Population&quot;)), points = list(col = trellis.par.get(&quot;superpose.symbol&quot;)$col[1:3], pch = trellis.par.get(&quot;superpose.symbol&quot;)$pch[1:3])) ) ) Code # Fitting a quadratic trend m3 &lt;- lmer(ramusht ~ time + I(time^2) + (time + I(time^2) || subject), dat) summary(m3) dotplot(ranef(m3), scales = list(x = list(relation = &quot;free&quot;)))[[1]] Code anova(m2, m3) # Profiles pm3 &lt;- profile(m3, which = &quot;theta_&quot;) xyplot(log(pm3)) Code densityplot(log(pm3)) Code splom(log(pm3)) Code confint(pm3) "],["crossed-random-effects.html", "Chapter 7 Crossed random effects 7.1 Slides 7.2 Exercises", " Chapter 7 Crossed random effects 7.1 Slides Unable to display PDF file. Download instead. 7.2 Exercises Exercise 1 Change the data simulation by Baayen et al. (2008) for \\(N = 30\\) subjects instead of only 3 You can use the following script and adjust it accordingly You can choose if you want to use model matrices or create the vectors “manually” Code library(lattice) library(lme4) #--------------- (1) Create data frame --------------- datsim &lt;- expand.grid(subject = factor(c(&quot;s1&quot; , &quot;s2&quot; , &quot;s3&quot; )), item = factor(c(&quot;w1&quot; , &quot;w2&quot; , &quot;w3&quot; )), soa = factor(c(&quot;long&quot; , &quot;short&quot; ))) datsim &lt;- datsim |&gt; sort_by(~ subject) #--------------- (2) Define parameters --------------- beta0 &lt;- 522.11 beta1 &lt;- -18.89 sw &lt;- 21.1 sy0 &lt;- 23.89 sy1 &lt;- 9 ry &lt;- -1 se &lt;- 9.9 #--------------- (3) Create vectors and simulate data --------------- # Fixed effects b0 &lt;- rep(beta0, 18) b1 &lt;- rep(rep(c(0, beta1), each = 3), 3) # Draw random effects w &lt;- rep(rnorm(3, mean = 0, sd = sw), 6) e &lt;- rnorm(18, mean = 0, sd = se) # Bivariate normal distribution sig &lt;- matrix(c(sy0^2, ry * sy0 * sy1, ry * sy0 * sy1, sy1^2), 2, 2) y01 &lt;- mvtnorm::rmvnorm(3, mean = c(0, 0), sigma = sig) y0 &lt;- rep(y01[,1], each = 6) y1 &lt;- rep(c(0, y01[1,2], 0, y01[2,2], 0, y01[3,2]), each = 3) datsim$rt &lt;- b0 + b1 + w + y0 + y1 + e #--------------- (4) Simulate data using model matrices --------------- X &lt;- model.matrix( ~ soa, datsim) Z &lt;- model.matrix( ~ 0 + item + subject + subject:soa, datsim, contrasts.arg = list(subject = contrasts(datsim$subject, contrasts = FALSE))) # Fixed effects beta &lt;- c(beta0, beta1) # Random effects u &lt;- c(w = unique(w), y0 = y01[,1], y1 = y01[,2]) datsim$rt2 &lt;- X %*% beta + Z %*% u + e #--------------- (5) Visualize simulated data --------------- xyplot(rt ~ soa | subject, datsim, group = item, type = &quot;b&quot;, layout = c(3, 1)) Code n &lt;- 30 datsim &lt;- expand.grid(subject = factor(paste0(&quot;s&quot;, 1:n)), item = factor(c(&quot;w1&quot; , &quot;w2&quot; , &quot;w3&quot; )), soa = factor(c(&quot;long&quot; , &quot;short&quot; ))) datsim &lt;- datsim |&gt; sort_by(~ subject) beta0 &lt;- 522.11 beta1 &lt;- -18.89 sw &lt;- 21.1 sy0 &lt;- 23.89 sy1 &lt;- 9 ry &lt;- -1 se &lt;- 9.9 w &lt;- rnorm(3, mean = 0, sd = sw) e &lt;- rnorm(n * 6, mean = 0, sd = se) # Bivariate normal distribution sig &lt;- matrix(c(sy0^2, ry * sy0 * sy1, ry * sy0 * sy1, sy1^2), 2, 2) y01 &lt;- mvtnorm::rmvnorm(n, mean = c(0, 0), sigma = sig) beta &lt;- c(beta0, beta1) # Random effects u &lt;- c(w = w, y0 = y01[,1], y1 = y01[,2]) X &lt;- model.matrix( ~ soa, datsim) Z &lt;- model.matrix( ~ 0 + item + subject + subject:soa, datsim, contrasts.arg = list(subject = contrasts(datsim$subject, contrasts = FALSE))) datsim$rt &lt;- X %*% beta + Z %*% u + e xyplot(rt ~ soa | subject, datsim, group = item, type = &quot;b&quot;, layout=c(5, 6)) Exercise 2 Fit the following models to the healing data by Aungle &amp; Langer (2023) Code load(&quot;data/healing.RData&quot;) m1 &lt;- lmer(Healing ~ Condition + (1 | Subject) + (1 | ResponseId), dat) m2 &lt;- lmer(Healing ~ Condition + (Condition | Subject) + (1 | ResponseId), dat) m3 &lt;- lmer(Healing ~ Condition + (1 | Subject) + (0 + dummy(Condition, &quot;28&quot;) | Subject) + (0 + dummy(Condition, &quot;56&quot;) | Subject) + (1 | ResponseId), dat) Profile the models with profile(&lt;model&gt;) Use the functions xyplot(), densityplot(), splom() from the lattice package to take a closer look at the estimated random parameters Compare the three models with likelihood ratio tests What is the best model in your opinion? Code pm1 &lt;- profile(m1) pm2 &lt;- profile(m2) pm3 &lt;- profile(m3) xyplot(pm1, which = &quot;theta_&quot;) Code xyplot(pm2, which = &quot;theta_&quot;) Code xyplot(pm3, which = &quot;theta_&quot;) Code densityplot(pm1, which = &quot;theta_&quot;) Code densityplot(pm2, which = &quot;theta_&quot;) Code densityplot(pm3, which = &quot;theta_&quot;) Code splom(pm1, which = &quot;theta_&quot;) Code splom(pm2, which = &quot;theta_&quot;) Code splom(pm3, which = &quot;theta_&quot;) Code anova(m1, m3, m2) References Aungle, P. &amp; Langer, E. (2023). Physical healing as a function of perceived time. Scientific Reports, 13(1), 22432. https://doi.org/10.1038/s41598-023-50009-3 Baayen, R. H., Davidson, D. J. &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005 "],["random-effects-for-within-subject-designs.html", "Chapter 8 Random effects for within-subject designs 8.1 Slides 8.2 Exercises", " Chapter 8 Random effects for within-subject designs 8.1 Slides Unable to display PDF file. Download instead. 8.2 Exercises Exercise 1 Recreate the data simulations presented on the slides Plot the confidence intervals for a 2x2 within-design for one observation per subject per cell and for 5 observations per subject per cell (cf. slides 20 and 22) How do the possible random effects for these models differ? Calculate the power for the different designs from your simulated data Code library(lme4) ############################################################################### sim_cis &lt;- function(form1, form2, data, params, test_par, nsim = 100) { cis &lt;- replicate(nsim, { y &lt;- simulate(formula(gsub(&quot;^y&quot;, &quot;&quot;, form2)), newdata = data, newparams = params)$sim_1 m0 &lt;- lmer(formula(form1), dat) m1 &lt;- lmer(formula(form2), dat) matrix(c(confint(m0, par = test_par, method = &quot;Wald&quot;) |&gt; as.numeric(), confint(m1, par = test_par, method = &quot;Wald&quot;) |&gt; as.numeric()), nrow = 2, byrow = TRUE) }, simplify = FALSE ) dat_ci &lt;- as.data.frame(do.call(rbind, cis)) names(dat_ci) &lt;- c(&quot;lb&quot;, &quot;ub&quot;) dat_ci$true_par &lt;- tail(params$beta, 1) dat_ci$model &lt;- factor(c(&quot;random intercept&quot;, &quot;random slope&quot;)) dat_ci } plot_cis &lt;- function(data) { plot(I(1:nrow(data)) ~ lb, data, type = &quot;n&quot;, xlim = c(-3, 3), xlab = &quot;&quot;, ylab = &quot;&quot;) arrows(data$lb, 1:nrow(data), data$ub, 1:nrow(data), code = 3, angle = 90, length = 0.05, col = c(&quot;gray&quot;, &quot;lightblue&quot;)[data$model]) with( aggregate(cbind(lb, ub) ~ model, data, mean), arrows(lb, c(nrow(data) / 2, nrow(data) / 2 + 2), ub, c(nrow(data) / 2, nrow(data) / 2 + 2), code = 3, angle = 90, length = 0.05, col = c(&quot;black&quot;, &quot;blue&quot;), lwd = 3) ) abline(v = c(0, unique(data$true_par)), lty = c(3, 1)) legend(&quot;topleft&quot;, c(&quot;random intercept&quot;, &quot;random slope&quot;), lty = 1, col = c(&quot;black&quot;, &quot;blue&quot;), lwd = 3, bty = &quot;n&quot;, cex = .9) } ############################################################################### nsubj &lt;- 10 nitem &lt;- 5 #----- 2x2 within design ------------------------------------------------------ dat &lt;- expand.grid(A = factor(c(&quot;a1&quot;, &quot;a2&quot;)), B = factor(c(&quot;b1&quot;, &quot;b2&quot;)), id = factor(1:nsubj)) # y = mu + a2 + b2 + a2b2 + p + pa + pb + e beta &lt;- c(3, .5, .5, 1) sp &lt;- c(1, .8, .6) r &lt;- -.5 se &lt;- 1 S &lt;- r * sp %o% sp; diag(S) &lt;- sp^2 Lt &lt;- chol(S) / se theta &lt;- t(Lt)[lower.tri(Lt, diag = TRUE)] dat_ci &lt;- sim_cis(&quot;y ~ A * B + (1 | id)&quot;, &quot;y ~ A * B + (A + B | id)&quot;, data = dat, params = list(beta = beta, theta = theta, sigma = se), test_par = &quot;Aa2:Bb2&quot;) pdf(&quot;slides/figures/nico_cis_2x2design_1obs.pdf&quot;, width = 3.375, height = 3.375, pointsize = 10) par(mai = c(.6, .6, .2, .1), mgp = c(2.4, 1, 0)) plot_cis(dat_ci) dev.off() # Power mean(dat_ci$lb[dat_ci$model == &quot;random intercept&quot;] &gt; 0) mean(dat_ci$lb[dat_ci$model == &quot;random slope&quot;] &gt; 0) #----- crossed random effects ------------------------------------------------- dat &lt;- expand.grid(A = factor(c(&quot;a1&quot;, &quot;a2&quot;)), B = factor(c(&quot;b1&quot;, &quot;b2&quot;)), item = factor(1:nitem), id = factor(1:nsubj)) # y = mu + a2 + b2 + a2b2 + p + pa + pb + papb + w + e sp &lt;- c(.4, .8, .6, .4) r &lt;- -.2 S &lt;- r * sp %o% sp; diag(S) &lt;- sp^2 Lt &lt;- chol(S) / se theta &lt;- t(Lt)[lower.tri(Lt, diag = TRUE)] sw &lt;- 1 theta2 &lt;- c(theta, sw / se) dat_ci &lt;- sim_cis(&quot;y ~ A * B + (1 | id) + (1 | item)&quot;, &quot;y ~ A * B + (A * B | id) + (1 | item)&quot;, data = dat, params = list(beta = beta, theta = theta2, sigma = se), test_par = &quot;Aa2:Bb2&quot;) pdf(&quot;slides/figures/nico_cis_2x2design_5obs.pdf&quot;, width = 3.375, height = 3.375, pointsize = 10) par(mai = c(.6, .6, .2, .1), mgp = c(2.4, 1, 0)) plot_cis(dat_ci) dev.off() # Power mean(dat_ci$lb[dat_ci$model == &quot;random intercept&quot;] &gt; 0) mean(dat_ci$lb[dat_ci$model == &quot;random slope&quot;] &gt; 0) Exercise 2 Adjust the data simulation for a design with a single within factor with three levels Look again at one model with three observations per subject (one observation per cell) and 15 observations per subject (5 observations per cell) How do the possible random effects for these models differ? "],["contrast-coding.html", "Chapter 9 Contrast coding 9.1 Slides 9.2 Exercises", " Chapter 9 Contrast coding 9.1 Slides Unable to display PDF file. Download instead. 9.2 Exercises Exercise 1 Simulate data based on the example by Brehm &amp; Alday (2022) for a 2x2 between-subjects design Code library(lme4) set.seed(1012) # Average time to eat in min SpoonSoup &lt;- 5 ForkSoup &lt;- 10 SpoonSalad &lt;- 10 ForkSalad &lt;- 5 # Standard deviation for all groups Groupsd &lt;- 2 # Number of subjects (ps) and ingredients (ii) - crossed random design ps &lt;- 20 ii &lt;- 10 ds &lt;- data.frame( Utensils = factor(rep(c(&quot;Spoon&quot;, &quot;Fork&quot;), each = ps * ii, times = 2)), Foods = factor(rep(c(&quot;Soup&quot;, &quot;Salad&quot;), each = ps * ii * 2)), Participant = factor(rep(paste0(&quot;p&quot;, 1:ps), times = ii * 4)), Item = factor(rep(paste0(&quot;i&quot;, 1:ii), each = ps, times = 4))) ds$RT &lt;- c(rnorm(ps * ii, mean = SpoonSoup, sd = Groupsd), rnorm(ps * ii, mean = ForkSoup, sd = Groupsd), rnorm(ps * ii, mean = SpoonSalad, sd = Groupsd), rnorm(ps * ii, mean = ForkSalad, sd = Groupsd)) psre &lt;- rnorm(ps, mean = 0, sd = Groupsd / 5) iire &lt;- rnorm(ii, mean = 0, sd = Groupsd / 5) ds$RT &lt;- ds$RT + psre[ds$Participant] + iire[ds$Item] Fit the following model to your simulated data \\[ RT = \\beta_0 + \\beta_1 UtensilsSpoon + \\beta_2 FoodsSoup + \\beta_3 UtensilsSpoon \\times FoodsSoup + \\upsilon_0 + \\eta_0 + \\varepsilon \\] with \\(\\upsilon_0 \\sim N(0, \\sigma_{\\upsilon}^2)\\), \\(\\eta_0 \\sim N(0, \\sigma_{\\eta}^2)\\), \\(\\varepsilon \\sim N(0, \\sigma_{\\varepsilon}^2)\\), all i.i.d. with dummy coding with effects coding Code # Treatment coding m1 &lt;- lmer(RT ~ Utensils * Foods + (1 | Participant) + (1 | Item), data = ds) summary(m1) # Effects coding contrasts(ds$Utensils) &lt;- contr.sum(levels(ds$Utensils)) contrasts(ds$Foods) &lt;- contr.sum(levels(ds$Foods)) m2 &lt;- lmer(RT ~ Utensils * Foods + (1 | Participant) + (1 | Item), data = ds) summary(m2) Interpret the fixed parameters and compare the variance components for the random effects for the two models What is the interpretation of the intercept for the two models? Why does it differ? Exercise 2 Fit linear models with all contrasts we looked at Treatment contrasts Sum contrasts Helmert contrasts Sequential difference contrasts Custom contrasts Interpret the fixed parameters Predict data with each model and compare the results Look again at the intercepts for all models and compare References Brehm, L. &amp; Alday, P. M. (2022). Contrast coding choices in a decade of mixed models. Journal of Memory and Language, 125, 104334. https://doi.org/10.1016/j.jml.2022.104334 "],["multilevel-models.html", "Chapter 10 Multilevel models 10.1 Slides 10.2 Exercises", " Chapter 10 Multilevel models 10.1 Slides Unable to display PDF file. Download instead. 10.2 Exercises Exercise 1 Load the jsp data set from the faraway package For simplicity, let us just consider the first measurement for each pupil (select a subset of the data for year == 0) Fit the following model to the jsp data set \\[\\begin{align*} \\text{(Level 1)} \\quad y_{ij} &amp;= b_{0i} + b_{1i}\\,gcraven_{ij} + b_{2i}\\,social_{ij} + \\varepsilon_{ij}\\\\ \\text{(Level 2)} \\quad b_{0i} &amp;= \\beta_0 + \\beta_3\\,mraven_i + \\upsilon_{0i} \\\\ \\quad b_{1i} &amp;= \\beta_1 + \\beta_4\\,mraven_i + \\upsilon_{1i}\\\\ \\quad b_{2i} &amp;= \\beta_2\\\\ \\text{(2) in (1)} \\quad y_{ij} &amp;= \\beta_{0} + \\beta_{1}\\,gcraven_{ij} + \\beta_{2}\\,social_{ij} + \\beta_{3}\\,mraven_i\\\\ &amp;~~~ + \\beta_{4}\\,(gcraven_{ij} \\times mraven_{i})\\\\ &amp;~~~ + \\upsilon_{0i} + \\upsilon_{1i}\\,gcraven_{ij} + \\varepsilon_{ij} \\end{align*}\\] with \\(\\boldsymbol\\upsilon \\sim N(\\boldsymbol 0, \\boldsymbol{\\Sigma}_\\upsilon)\\) i.i.d., \\(\\varepsilon_{ij} \\sim N(0, \\sigma^2)\\) i.i.d. Profile the model with profile(&lt;model&gt;) Use the functions xyplot(), densityplot(), splom() from the lattice package to take a closer look at the estimated random parameters What’s your conclusion about the correlation parameter? Code library(lme4) library(lattice) data(&quot;jsp&quot;, package = &quot;faraway&quot;) dat &lt;- subset(jsp, year == 0) # Centering around grand mean dat$craven &lt;- dat$raven - mean(dat$raven) # Centering around group mean for each school ## add mean raven score per school dat$mraven &lt;- with(dat, ave(raven, school)) dat$mraven &lt;- dat$mraven - mean(dat$mraven) ## center raven score: mean = 0 for each school dat$gcraven &lt;- dat$craven - dat$mraven # Fit model m &lt;- lmer(math ~ mraven * gcraven + social + (gcraven | school), data = dat, REML = FALSE) pm &lt;- profile(m) xyplot(pm) Code #densityplot(pm) splom(pm, which = &quot;theta_&quot;) Code m_zcor &lt;- lmer(math ~ mraven * gcraven + social + (gcraven || school), data = dat, REML = FALSE) anova(m_zcor, m) "],["generalized-linear-mixed-effects-models.html", "Chapter 11 Generalized linear mixed-effects models 11.1 Slides 11.2 Exercises", " Chapter 11 Generalized linear mixed-effects models 11.1 Slides Unable to display PDF file. Download instead. 11.2 Exercises Exercise 1 Load the example data from https://github.com/eveskew/glmm_tutorial Code # Download the data from https://github.com/eveskew/glmm_tutorial dat &lt;- read.csv(&quot;https://raw.githubusercontent.com/eveskew/glmm_tutorial/refs/heads/master/glmm_tutorial_data.csv&quot;) The data set contains data on 11 studies investigating how many animals got tested positive for a certain disease (sorry, I could not find out which one) Additional variables are Habitat indicating if the animals were tested in a forest (Forest) or in a agricultural context (Ag) and Season with levels Spring and Fall Create a plot with prevalence on the y-axis, habitat on the x-axis, two panels for season and one line for each study Fit GLMMs predicting the probability that an animal is tested positive using Season and Habitat as predictors What would you choose as random effect here? Which model would you choose? Interpret your results Code library(lme4) dat$Habitat &lt;- factor(dat$Habitat, levels = c(&quot;Forest&quot;, &quot;Ag&quot;)) dat$Season &lt;- factor(dat$Season) dat$Study &lt;- factor(dat$Study) dat$Prev &lt;- dat$Positive / dat$Num_Tested # Plot data lattice::xyplot(Prev ~ Habitat | Season, data = dat, type = &quot;b&quot;, groups = Study) Code # Fit GLMMs gm1 &lt;- glmer(cbind(Positive, Num_Tested - Positive) ~ 1 + (1 | Study), data = dat, family = binomial) gm2 &lt;- glmer(cbind(Positive, Num_Tested - Positive) ~ Habitat + (1 | Study), data = dat, family = binomial) gm3 &lt;- glmer(cbind(Positive, Num_Tested - Positive) ~ Habitat + Season + (1 | Study), data = dat, family = binomial) gm4 &lt;- glmer(cbind(Positive, Num_Tested - Positive) ~ Habitat * Season + (1 | Study), data = dat, family = binomial) anova(gm1, gm2, gm3, gm4) # Odds ratio for Habitat exp(fixef(gm2))[2] "],["data-simulation-in-r.html", "Chapter 12 Data simulation in R 12.1 Overview 12.2 Random numbers generation 12.3 Creating factors 12.4 Data frames 12.5 Simulate repeatedly 12.6 Example: Type I error 12.7 Reference", " Chapter 12 Data simulation in R In this session, we will use R to simulate data sets. Simulating data can be considered as the key idea behind hypothesis testing in the frequentist framework, where we assume that we can repeat the data collection under identical conditions. 12.1 Overview Random numbers generation (rnorm(), runif(), rpois(), rbinom(), etc.) Generate categorical variables (factor(), rep()) Create data frames containing quantitative and categorical variables (data.frame(), expand.grid(), reshape()) Drawing many data sets (replicate(), for loops) 12.2 Random numbers generation For drawing random numbers from a statistical distribution, the distribution name is prefixed by “r” (random deviate). See ?Distributions for a list of distributions. Code rnorm(10) # draw from standard normal distribution ## [1] -1.57414237 0.64583220 -1.41354749 0.74087481 -1.07617875 1.40767808 0.68572922 ## [8] 0.34442130 0.07640937 -0.74282934 Code runif(10) # draw from uniform distribution ## [1] 0.665568487 0.717326792 0.432359260 0.985922601 0.946721491 0.628644241 0.007869274 ## [8] 0.371857279 0.644679366 0.999616833 Code rpois(10, lambda = 1) # draw from Poisson distribution ## [1] 1 2 3 0 2 0 1 2 1 1 Code # Sampling with or without replacement from a vector sample(1:5, size = 10, replace = TRUE) ## [1] 5 4 4 4 1 2 1 2 2 3 The random numbers generator in R is seeded: Upon restart of R, new random numbers are generated. To replicate the results of a simulation, the seed (starting value) can be set explicitly: Code set.seed(1223) # set seed, so on each run random numbers will be identical runif(3) ## [1] 0.6289619 0.1267469 0.3285822 12.2.1 Normal distribution The normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\): \\(X \\sim N(\\mu, \\sigma^2)\\). Code rnorm(100, mean = 100, sd = 15) |&gt; hist(breaks = 30) 12.2.2 Multivariate normal distribution Code MASS::mvrnorm(100, mu = c(0, 0), Sigma = matrix(c(1, 0.5, 0.5, 1), nrow = 2, ncol = 2)) |&gt; plot(xlab = &quot;x1&quot;, ylab = &quot;x2&quot;) 12.2.3 Poisson distribution (for count data) The Poisson distribution is a discrete probability distribution that is used to model the probability of a given number of events occurring in a fixed interval of time if these events occur with a known constant mean rate (\\(\\lambda\\)) and independently of the time since the last event. Code rpois(100, lambda = 2.5) |&gt; hist() 12.2.4 Binomial distribution The binomial distribution with parameters \\(n\\) and \\(\\pi\\) is the discrete probability distribution of the number of successes in a sequence of n independent (Bernoulli) experiments: \\(X \\sim Binom(n, \\pi)\\). Generate data from a binomial model using the function rbinom() in R; try out different values of – \\(n\\) (10, 500, 2000) – the parameter \\(\\pi\\) (0.5, 0.8, 0.44, 0.515) and see how this affects the output. Code n &lt;- 10 # 500, 2000 p &lt;- 0.5 # 0.8, 0.44, 0.515 rbinom(100, size = n, prob = p) |&gt; table() |&gt; plot() With these data, test different null hypotheses using binom.test(); these may or may not coincide with the values of \\(\\pi\\) used for data generation. Code x &lt;- rbinom(1, size = 10, prob = 0.3) binom.test(x, n = 10, p = 0.5) ## ## Exact binomial test ## ## data: x and 10 ## number of successes = 1, number of trials = 10, p-value = 0.02148 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.002528579 0.445016117 ## sample estimates: ## probability of success ## 0.1 If you repeat data generation and testing, can you usually reject H0? Code pval &lt;- replicate(500, { x &lt;- rbinom(1, size = 50, prob = 0.3) binom.test(x, n = 50, p = 0.5)$p.value } ) mean(pval &lt; 0.05) ## [1] 0.778 12.3 Creating factors It is usually good practice to create categorical variables explicitly as factors. Everything that is not a numeric variable should be a factor (e.g., id variables). Code sex &lt;- factor(rep(c(&quot;male&quot;, &quot;female&quot;), c(15, 20)), levels = c(&quot;male&quot;, &quot;female&quot;, &quot;diverse&quot;)) sex ## [1] male male male male male male male male male male male male ## [13] male male male female female female female female female female female female ## [25] female female female female female female female female female female female ## Levels: male female diverse Code condition &lt;- factor(rep(1:2, 20), levels = 1:2, labels = c(&quot;real&quot;, &quot;VR&quot;)) condition ## [1] real VR real VR real VR real VR real VR real VR real VR real VR real ## [18] VR real VR real VR real VR real VR real VR real VR real VR real VR ## [35] real VR real VR real VR ## Levels: real VR Code group &lt;- factor(rep(c(&quot;ctr&quot;, &quot;trt1&quot;, &quot;trt2&quot;), each = 5)) group ## [1] ctr ctr ctr ctr ctr trt1 trt1 trt1 trt1 trt1 trt2 trt2 trt2 trt2 trt2 ## Levels: ctr trt1 trt2 The levels argument sets explicitly the ordering of the factor levels. In dummy coding (default in R) the first factor level is taken as the reference category. Code model.matrix( ~ group) ## (Intercept) grouptrt1 grouptrt2 ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 1 0 ## 7 1 1 0 ## 8 1 1 0 ## 9 1 1 0 ## 10 1 1 0 ## 11 1 0 1 ## 12 1 0 1 ## 13 1 0 1 ## 14 1 0 1 ## 15 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$group ## [1] &quot;contr.treatment&quot; Code contrasts(group) ## trt1 trt2 ## ctr 0 0 ## trt1 1 0 ## trt2 0 1 12.4 Data frames When simulating data for a certain experimental design, this is reflected in the structure of your data frame. For the group variable from above, the design looks like this Code model.matrix( ~ group) |&gt; unique() ## (Intercept) grouptrt1 grouptrt2 ## 1 1 0 0 ## 6 1 1 0 ## 11 1 0 1 A linear model will make use of this design and estimate three parameters: \\[\\begin{align*} \\beta_0 &amp; = \\text{mean of control group} \\\\ \\beta_1 &amp; = \\text{effect of treatment group 1} \\\\ \\beta_2 &amp; = \\text{effect of treatment group 2} \\\\ \\end{align*}\\] For a repeated-measures (within-subjects) design, the data frame will usually be in a long format. Code n &lt;- 20 # A and B are recycled to match the length of id datsim &lt;- data.frame(id = factor(rep(1:n, each = 4)), A = factor(rep(c(&quot;a1&quot;, &quot;a2&quot;), each = 2)), B = factor(rep(c(&quot;b1&quot;, &quot;b2&quot;), times = 2))) xtabs( ~ A + B, datsim) ## B ## A b1 b2 ## a1 20 20 ## a2 20 20 Code xtabs( ~ id + A + B, datsim) |&gt; ftable() ## B b1 b2 ## id A ## 1 a1 1 1 ## a2 1 1 ## 2 a1 1 1 ## a2 1 1 ## 3 a1 1 1 ## a2 1 1 ## 4 a1 1 1 ## a2 1 1 ## 5 a1 1 1 ## a2 1 1 ## 6 a1 1 1 ## a2 1 1 ## 7 a1 1 1 ## a2 1 1 ## 8 a1 1 1 ## a2 1 1 ## 9 a1 1 1 ## a2 1 1 ## 10 a1 1 1 ## a2 1 1 ## 11 a1 1 1 ## a2 1 1 ## 12 a1 1 1 ## a2 1 1 ## 13 a1 1 1 ## a2 1 1 ## 14 a1 1 1 ## a2 1 1 ## 15 a1 1 1 ## a2 1 1 ## 16 a1 1 1 ## a2 1 1 ## 17 a1 1 1 ## a2 1 1 ## 18 a1 1 1 ## a2 1 1 ## 19 a1 1 1 ## a2 1 1 ## 20 a1 1 1 ## a2 1 1 Code # OR id &lt;- factor(1:n) A &lt;- factor(c(&quot;a1&quot;, &quot;a2&quot;)) B &lt;- factor(c(&quot;b1&quot;, &quot;b2&quot;)) datsim2 &lt;- expand.grid(id = id, A = A, B = B) |&gt; sort_by(~ id + A) We can transform the data frame between long and wide data format with reshape(). Code datl &lt;- data.frame(id = factor(rep(1:n, each = 7)), time = rep(0:6, times = n), resp = rnorm(n * 7, mean = seq(1, 5, length.out = 7), # seven means sd = 1)) aggregate(resp ~ time, datl, mean) ## time resp ## 1 0 1.034856 ## 2 1 1.648281 ## 3 2 2.334861 ## 4 3 2.732789 ## 5 4 3.600051 ## 6 5 3.842304 ## 7 6 5.031247 Code datw &lt;- reshape(datl, direction = &quot;wide&quot;, idvar = &quot;id&quot;, timevar = &quot;time&quot;) colMeans(datw[, -1]) ## resp.0 resp.1 resp.2 resp.3 resp.4 resp.5 resp.6 ## 1.034856 1.648281 2.334861 2.732789 3.600051 3.842304 5.031247 Code cor(datw[, -1]) ## resp.0 resp.1 resp.2 resp.3 resp.4 resp.5 resp.6 ## resp.0 1.00000000 -0.1515961 0.09738831 -0.32935371 -0.3494126 0.05470633 -0.23307043 ## resp.1 -0.15159614 1.0000000 0.54631105 0.12950803 -0.2360399 0.38303961 0.32874890 ## resp.2 0.09738831 0.5463110 1.00000000 -0.04246943 -0.3037968 0.05777371 0.12258577 ## resp.3 -0.32935371 0.1295080 -0.04246943 1.00000000 0.1153210 0.13318325 0.09966404 ## resp.4 -0.34941260 -0.2360399 -0.30379682 0.11532103 1.0000000 -0.42137211 -0.20102584 ## resp.5 0.05470633 0.3830396 0.05777371 0.13318325 -0.4213721 1.00000000 0.15399528 ## resp.6 -0.23307043 0.3287489 0.12258577 0.09966404 -0.2010258 0.15399528 1.00000000 12.5 Simulate repeatedly Reading material on functional programming in R (Wickham, 2019) http://adv-r.had.co.nz/Functions.html http://adv-r.had.co.nz/Functional-programming.html http://adv-r.had.co.nz/Functionals.html Code set.seed(1133) replicate(3, rnorm(10), simplify = FALSE) ## [[1]] ## [1] 1.3234050 1.0672355 -0.4970425 -0.7444275 0.1860472 1.0540061 0.9051997 0.5745372 ## [9] 2.0237698 0.1831237 ## ## [[2]] ## [1] 0.25400660 -0.71355882 1.44786556 0.90886066 -2.02238191 0.27064121 -0.97261079 ## [8] 0.01889436 2.08511877 -0.88604130 ## ## [[3]] ## [1] 0.3268653 1.4123795 -0.4843307 2.5777461 0.2671252 -0.9092667 -2.3162740 0.9670641 ## [9] -1.5804482 0.3733937 Code set.seed(1133) replicate(3, rnorm(10)) ## [,1] [,2] [,3] ## [1,] 1.3234050 0.25400660 0.3268653 ## [2,] 1.0672355 -0.71355882 1.4123795 ## [3,] -0.4970425 1.44786556 -0.4843307 ## [4,] -0.7444275 0.90886066 2.5777461 ## [5,] 0.1860472 -2.02238191 0.2671252 ## [6,] 1.0540061 0.27064121 -0.9092667 ## [7,] 0.9051997 -0.97261079 -2.3162740 ## [8,] 0.5745372 0.01889436 0.9670641 ## [9,] 2.0237698 2.08511877 -1.5804482 ## [10,] 0.1831237 -0.88604130 0.3733937 Code # Using a for loop ## Create an empty list l1 &lt;- vector(mode = &quot;list&quot;, length = 3) ## Loop to add elements to the list for(i in 1:3) { l1[[i]] &lt;- rnorm(10) } l1 ## [[1]] ## [1] -1.4462982 -1.0249066 1.7273781 0.7970825 -0.1921147 0.2560200 0.7947771 -0.4665010 ## [9] 0.5127336 -0.4892820 ## ## [[2]] ## [1] 1.873255506 -0.450625527 -1.060191680 -0.410976388 -0.631345739 -0.646261755 ## [7] -0.347821457 0.002526977 -1.474943882 -0.903203170 ## ## [[3]] ## [1] -0.5318484 -0.9318605 0.6605868 -1.3563084 -1.4592700 -0.3774795 0.1023129 -0.2826929 ## [9] -0.4166077 0.1255623 Code ## Grow a matrix mat &lt;- NULL for(i in 1:3) { mat &lt;- cbind(mat, rnorm(10)) } mat ## [,1] [,2] [,3] ## [1,] -1.27717070 -0.1744192 -0.09960859 ## [2,] 0.08109145 -1.1686442 -0.06927740 ## [3,] 1.46141004 -0.1793560 0.20608944 ## [4,] 1.03355461 -0.0960317 -2.17113052 ## [5,] -0.31967339 -1.4917276 0.52429185 ## [6,] 0.37209221 -1.1401370 0.04022837 ## [7,] 1.80480805 -0.2392017 -0.10797109 ## [8,] 0.69115420 1.8640513 1.35247834 ## [9,] 1.93660079 -1.3204190 -0.25169725 ## [10,] 0.56133067 1.3491119 0.57756123 12.5.1 Simulate many data sets Code # Create a list of data sets data_sets &lt;- replicate(20, { data.frame(id = factor(1:15), group = rep(c(&quot;ctr&quot;, &quot;trt1&quot;, &quot;trt2&quot;), times = 5), resp = rnorm(5 * 3, mean = c(1, 3, 5), # three means sd = 1)) }, simplify = FALSE ) data_sets[[1]] ## id group resp ## 1 1 ctr 0.18938344 ## 2 2 trt1 4.60991374 ## 3 3 trt2 6.49335383 ## 4 4 ctr 1.24668163 ## 5 5 trt1 1.97917586 ## 6 6 trt2 5.98611621 ## 7 7 ctr 0.53481673 ## 8 8 trt1 2.32272608 ## 9 9 trt2 5.13326355 ## 10 10 ctr -0.09014815 ## 11 11 trt1 1.61527749 ## 12 12 trt2 3.96499841 ## 13 13 ctr 1.11900065 ## 14 14 trt1 0.68836079 ## 15 15 trt2 6.13870532 Code # Use apply functions to fit model to each data set fit_model &lt;- function(data) { lm(resp ~ group, data = data) } models &lt;- lapply(data_sets, fit_model) # create list of fitted models summary(models[[1]]) ## ## Call: ## lm(formula = resp ~ group, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.57829 -0.51919 -0.06513 0.55724 2.36682 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.5999 0.4820 1.245 0.2370 ## grouptrt1 1.6431 0.6816 2.411 0.0329 * ## grouptrt2 4.9433 0.6816 7.252 1.01e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.078 on 12 degrees of freedom ## Multiple R-squared: 0.8197, Adjusted R-squared: 0.7897 ## F-statistic: 27.28 on 2 and 12 DF, p-value: 3.432e-05 Code # Use apply functions to extract parameters from each model pars &lt;- sapply(models, coef) rowMeans(pars) ## (Intercept) grouptrt1 grouptrt2 ## 0.7440095 2.2232360 4.2134159 Code boxplot(t(pars)) 12.6 Example: Type I error Code lm0 &lt;- lm(dist ~ 1, cars) # H0 lm1 &lt;- lm(dist ~ speed, cars) # H1 nsim &lt;- 1000 pval &lt;- numeric(nsim) for (i in 1:nsim) { sim &lt;- simulate(lm0)$sim_1 fit &lt;- lm(sim ~ speed, cars) pval[i] &lt;- summary(fit)$coef[&quot;speed&quot;, &quot;Pr(&gt;|t|)&quot;] } # Type I error mean(pval &lt; 0.05) ## [1] 0.04 12.7 Reference References Wickham, H. (2019). Advanced R (First edition). CRC press. http://adv-r.had.co.nz/ "],["power-simulation-for-lmms.html", "Chapter 13 Power simulation for LMMs 13.1 Reanalysis 13.2 Power simulation 13.3 References", " Chapter 13 Power simulation for LMMs Code library(lattice) library(lme4) 13.1 Reanalysis 13.1.1 Application context: Depression and type of diagnosis Reisby et al. (1977) studied the effect of Imipramin on 66 inpatients treated for depression Depression was measured with the Hamilton depression rating scale Patients were classified into endogenous and non-endogenous depressed Depression was measured weekly for 6 time points Data: reisby.dat Code dat &lt;- read.table(&quot;data/reisby.dat&quot;, header = TRUE) dat$id &lt;- factor(dat$id) dat$diag &lt;- factor(dat$diag, levels = c(&quot;nonen&quot;, &quot;endog&quot;)) dat &lt;- na.omit(dat) # drop missing values head(dat, n = 13) ## id hamd week diag endweek ## 1 101 26 0 nonen 0 ## 2 101 22 1 nonen 0 ## 3 101 18 2 nonen 0 ## 4 101 7 3 nonen 0 ## 5 101 4 4 nonen 0 ## 6 101 3 5 nonen 0 ## 7 103 33 0 nonen 0 ## 8 103 24 1 nonen 0 ## 9 103 15 2 nonen 0 ## 10 103 24 3 nonen 0 ## 11 103 15 4 nonen 0 ## 12 103 13 5 nonen 0 ## 13 104 29 0 endog 0 Code xyplot(hamd ~ week | id, data = dat, type=c(&quot;g&quot;, &quot;r&quot;, &quot;p&quot;), pch = 16, layout = c(11, 6), ylab = &quot;HDRS score&quot;, xlab = &quot;Time (week)&quot;) 13.1.2 Random-intercept model \\[ \\begin{aligned} Y_{ij} &amp;= \\beta_0 + \\beta_1 \\, \\mathtt{week}_{ij} + \\upsilon_{0i} + \\varepsilon_{ij} \\\\ \\upsilon_{0i} &amp;\\sim N(0, \\sigma^2_{\\upsilon_0}) \\text{ i.i.d.} \\\\ \\mathbf{\\varepsilon}_i &amp;\\sim N(0, \\, \\sigma^2) \\text{ i.i.d.} \\\\ i &amp;= 1, \\ldots, I, \\quad j = 1, \\ldots n_i \\end{aligned} \\] Code m1 &lt;- lmer(hamd ~ week + (1 | id), data = dat, REML = FALSE) summary(m1) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: hamd ~ week + (1 | id) ## Data: dat ## ## AIC BIC logLik -2*log(L) df.resid ## 2293.2 2308.9 -1142.6 2285.2 371 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.1739 -0.5876 -0.0342 0.5465 3.5297 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 16.16 4.019 ## Residual 19.04 4.363 ## Number of obs: 375, groups: id, 66 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 23.5518 0.6385 36.88 ## week -2.3757 0.1350 -17.60 ## ## Correlation of Fixed Effects: ## (Intr) ## week -0.524 13.1.3 Random-slope model \\[ \\begin{aligned} Y_{ij} &amp;= \\beta_0 + \\beta_1 \\, \\mathtt{week}_{ij} + \\upsilon_{0i} + \\upsilon_{1i}\\, \\mathtt{week}_{ij} + \\varepsilon_{ij} \\\\ \\begin{pmatrix} \\upsilon_{0i}\\\\ \\upsilon_{1i} \\end{pmatrix} &amp;\\sim N \\left(\\begin{pmatrix} 0\\\\ 0 \\end{pmatrix}, \\, \\mathbf{\\Sigma}_\\upsilon = \\begin{pmatrix} \\sigma^2_{\\upsilon_0} &amp; \\sigma_{\\upsilon_0 \\upsilon_1} \\\\ \\sigma_{\\upsilon_0 \\upsilon_1} &amp; \\sigma^2_{\\upsilon_1} \\\\ \\end{pmatrix} \\right) \\text{ i.i.d.} \\\\ \\mathbf{\\varepsilon}_i &amp;\\sim N(\\mathbf{0}, \\, \\sigma^2 \\mathbf{I}_{n_i}) \\text{ i.i.d.} \\\\ i &amp;= 1, \\ldots, I, \\quad j = 1, \\ldots n_i \\end{aligned} \\] Code m2 &lt;- lmer(hamd ~ week + (week | id), data = dat, REML = FALSE) summary(m2) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: hamd ~ week + (week | id) ## Data: dat ## ## AIC BIC logLik -2*log(L) df.resid ## 2231.0 2254.6 -1109.5 2219.0 369 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.7460 -0.5016 0.0332 0.5177 3.6834 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## id (Intercept) 12.631 3.554 ## week 2.079 1.442 -0.28 ## Residual 12.216 3.495 ## Number of obs: 375, groups: id, 66 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 23.5769 0.5456 43.22 ## week -2.3771 0.2087 -11.39 ## ## Correlation of Fixed Effects: ## (Intr) ## week -0.449 13.1.4 Partial pooling Code indiv &lt;- unlist( sapply(unique(dat$id), function(i) predict(lm(hamd ~ week, dat[dat$id == i, ]))) ) xyplot(hamd + predict(m2, re.form = ~ 0) + predict(m2) + indiv ~ week | id, data = dat, type = c(&quot;p&quot;, &quot;l&quot;, &quot;l&quot;, &quot;l&quot;), pch = 16, grid = TRUE, distribute.type = TRUE, layout = c(11, 6), ylab = &quot;HDRS score&quot;, xlab = &quot;Time (week)&quot;, # customize colors col = c(&quot;#434F4F&quot;, &quot;#3CB4DC&quot;, &quot;#FF6900&quot;, &quot;#78004B&quot;), # add legend key = list(space = &quot;top&quot;, columns = 3, text = list(c(&quot;Population&quot;, &quot;Mixed model&quot;, &quot;Within-subject&quot;)), lines = list(col = c(&quot;#3CB4DC&quot;, &quot;#FF6900&quot;, &quot;#78004B&quot;))) ) 13.1.5 By-group random-slope model Code m3 &lt;- lmer(hamd ~ week + diag + (week | id), data = dat, REML = FALSE) m4 &lt;- lmer(hamd ~ week * diag + (week | id), data = dat, REML = FALSE) anova(m3, m4) ## Data: dat ## Models: ## m3: hamd ~ week + diag + (week | id) ## m4: hamd ~ week * diag + (week | id) ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## m3 7 2228.9 2256.4 -1107.5 2214.9 ## m4 8 2230.9 2262.3 -1107.5 2214.9 0.0042 1 0.9486 13.1.6 Means and predicted HDRS score by group Code dat2 &lt;- aggregate(hamd ~ week + diag, dat, mean) dat2$m4 &lt;- predict(m4, newdata = dat2, re.form = ~ 0) plot(m4 ~ week, dat2[dat2$diag == &quot;endog&quot;, ], type = &quot;l&quot;, ylim=c(0, 28), xlab=&quot;Week&quot;, ylab = &quot;HDRS score&quot;) lines(m4 ~ week, dat2[dat2$diag == &quot;nonen&quot;, ], lty = 2) points(hamd ~ week, dat2[dat2$diag == &quot;endog&quot;, ], pch = 16) points(hamd ~ week, dat2[dat2$diag == &quot;nonen&quot;, ], pch = 21, bg = &quot;white&quot;) legend(&quot;topright&quot;, c(&quot;Endogenous&quot;, &quot;Non endogenous&quot;), lty = 1:2, pch = c(16, 21), pt.bg = &quot;white&quot;, bty = &quot;n&quot;) 13.2 Power simulation 13.2.1 Setup Code ## Study design and sample sizes n_week &lt;- 6 n_subj &lt;- 80 n &lt;- n_week * n_subj dat &lt;- data.frame( id = factor(rep(seq_len(n_subj), each = n_week)), week = rep(0:(n_week - 1), times = n_subj), treat = factor(rep(0:1, each = n/2), labels = c(&quot;ctr&quot;, &quot;trt&quot;)) ) ## Fixed effects and variance components beta &lt;- c(&quot;(Intercept)&quot; = 23, week = -0.5, treattrt = 0, &quot;week:treattrt&quot; = -1) se &lt;- 3.5 # residual sd r &lt;- -0.3 t(chol(VarCorr(m4)$id))[lower.tri(diag(2), diag = TRUE)] / sigma(m3) ## [1] 0.9760902 -0.1175204 0.3952024 Code # su &lt;- c(3.5, 1.5) # Su &lt;- r * su %o% su # diag(Su) &lt;- su^2 # covariance matrix of random effects su1 &lt;- 3.5 su2 &lt;- 1.5 Su &lt;- matrix(c(su1^2, r * su1 * su2, r * su1 * su2, su2^2), nrow = 2, ncol = 2) 13.2.2 Power 13.2.2.1 Simulate data with bivariate normal distribution Code pval &lt;- replicate(200, { # Data generation means &lt;- model.matrix( ~ week * treat, dat) %*% beta ranu &lt;- MASS::mvrnorm(n_subj, mu = c(0, 0), Sigma = Su) e &lt;- rnorm(n_subj * n_week, mean = 0, sd = se) y &lt;- means + ranu[dat$id, 1] + ranu[dat$id, 2] * dat$week + e # Fitting model to test H0 m0 &lt;- lmer(y ~ week + treat + (1 + week | id), data = dat, REML = FALSE) m1 &lt;- lmer(y ~ week * treat + (1 + week | id), data = dat, REML = FALSE) anova(m0, m1)[&quot;m1&quot;, &quot;Pr(&gt;Chisq)&quot;] } ) mean(pval &lt; 0.05) ## [1] 0.785 13.2.2.2 Simulate data with lme4::simulate() Code ## Gory details fixef(m4) ## (Intercept) week diagendog week:diagendog ## 22.47626332 -2.36568746 1.98802087 -0.02705576 Code getME(m4, &quot;theta&quot;) ## id.(Intercept) id.week.(Intercept) id.week ## 0.9760823 -0.1175194 0.3951992 Code t(chol(VarCorr(m4)$id))[lower.tri(diag(2), diag = TRUE)] / sigma(m4) ## [1] 0.9760823 -0.1175194 0.3951992 Code # Cholesky decomposition Lt &lt;- chol(Su) / se pars &lt;- list(theta = t(Lt)[lower.tri(Lt, TRUE)], beta = beta, sigma = se) names(pars$theta) &lt;- c(&quot;id.(Intercept)&quot;, &quot;id.week.(Intercept)&quot;, &quot;id.week&quot;) pval &lt;- replicate(200, { y &lt;- simulate(~ week * treat + (week | id), newparams = pars, newdata = dat)$sim_1 m1 &lt;- lmer(y ~ week + treat + (week | id), data = dat, REML = FALSE) m2 &lt;- lmer(y ~ week * treat + (week | id), data = dat, REML = FALSE) anova(m1, m2)$&quot;Pr(&gt;Chisq)&quot;[2] }) mean(pval &lt; 0.05) ## [1] 0.715 13.2.3 Parameter recovery Code par &lt;- replicate(200, { means &lt;- model.matrix( ~ week * treat, dat) %*% beta ranu &lt;- MASS::mvrnorm(n_subj, mu = c(0, 0), Sigma = Su) e &lt;- rnorm(n_subj * n_week, mean = 0, sd = se) y &lt;- means + ranu[dat$id, 1] + ranu[dat$id, 2] * dat$week + e m1 &lt;- lmer(y ~ week * treat + (1 + week | id), data = dat, REML = FALSE) list(fixef = fixef(m1), theta = getME(m1, &quot;theta&quot;), sigma = sigma(m1)) }, simplify = FALSE ) rowMeans(sapply(par, function(x) x$fixef)) ## (Intercept) week treattrt week:treattrt ## 22.98928760 -0.51150704 0.02675854 -0.97446036 Code rowMeans(sapply(par, function(x) x$theta)) ## id.(Intercept) id.week.(Intercept) id.week ## 0.9731571 -0.1223714 0.3978360 Code mean(sapply(par, function(x) x$sigma)) ## [1] 3.502159 Code beta ## (Intercept) week treattrt week:treattrt ## 23.0 -0.5 0.0 -1.0 Code Lt &lt;- chol(Su) t(Lt)[lower.tri(Lt, diag = TRUE)] / se ## [1] 1.0000000 -0.1285714 0.4088311 Code se ## [1] 3.5 13.3 References References Reisby, N., Gram, L. F., Bech, P., Nagy, A., Petersen, G. O., Ortmann, J., Ibsen, I., Dencker, S. J., Jacobsen, O., Krautwald, O., Sondergaard, I. &amp; Christiansen, J. (1977). Imipramine: Clinical effects and pharmacokinetic variability. Psychopharmacology, 54, 263–272. https://doi.org/10.1007/BF00426574 "],["power-simulation-for-glmms.html", "Chapter 14 Power simulation for GLMMs 14.1 Slides 14.2 Exercises", " Chapter 14 Power simulation for GLMMs 14.1 Slides TODO 14.2 Exercises TODO "],["references-1.html", "References", " References Alday, P., Kliegl, R. &amp; Bates, D. (2025). Embrace uncertainty – Mixed-effects models with Julia. https://embraceuncertaintybook.com/ Aungle, P. &amp; Langer, E. (2023). Physical healing as a function of perceived time. Scientific Reports, 13(1), 22432. https://doi.org/10.1038/s41598-023-50009-3 Baayen, R. H., Davidson, D. J. &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005 Bates, D., Mächler, M., Bolker, B. &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01 Brehm, L. &amp; Alday, P. M. (2022). Contrast coding choices in a decade of mixed models. Journal of Memory and Language, 125, 104334. https://doi.org/10.1016/j.jml.2022.104334 Gelman, A., Hill, J. &amp; Vehtari, A. (2020). Regression and other stories. Cambridge University Press. Hedeker, D. R. &amp; Gibbons, R. D. (2006). Longitudinal data analysis. John Wiley. Lowry, R. (2000). Concepts and applications of inferential statistics. http://vassarstats.net/textbook/ Reisby, N., Gram, L. F., Bech, P., Nagy, A., Petersen, G. O., Ortmann, J., Ibsen, I., Dencker, S. J., Jacobsen, O., Krautwald, O., Sondergaard, I. &amp; Christiansen, J. (1977). Imipramine: Clinical effects and pharmacokinetic variability. Psychopharmacology, 54, 263–272. https://doi.org/10.1007/BF00426574 Vasishth, S., Schad, D., Bürki, A. &amp; Kliegl, R. (2022). Linear mixed models in linguistics and psychology: A comprehensive introduction. https://vasishth.github.io/Freq_CogSci Wickelmaier, F. (2022). Simulating the power of statistical tests: A collection of R examples. ArXiv. https://arxiv.org/abs/2110.09836 Wickham, H. (2019). Advanced R (First edition). CRC press. http://adv-r.had.co.nz/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
