[["index.html", "Course on Mixed-effects Models Course information Desirable background knowledge Overview of dates Literature", " Course on Mixed-effects Models Nora Wickelmaier 2024-10-17 Course information This is an online book accompanying a course on mixed-effects models taught to PhD students at the Leibniz-Institut für Wissensmedien (IWM) in Tübingen. The data sets used in the examples and exercises can be downloaded here. So far, this is work in progress and very rudimentary! Desirable background knowledge Alle examples in this course are worked out in R. Introductory knowledge in R is therefore a prerequisite. If you want to freshen up your R knowledge a bit, you can go through the exercises provided here: https://nwickel.github.io/R_intro/. Apart from this, I will assume that you have some kind of workflow how to use R with a text editor (e.g., Vim) or an IDE (e.g., RStudio). It is further assumed that you have attended at least one introductory statistics course (level “Statisik I” and “Statistik II” usually taught in psychology curricula at German universities). That means you should be familiar with concepts like random variables, statistical distributions like the normal distribution, t distribution, Binomial distribution, etc., and hypothesis testing. Some background in regression analysis is helpful, but we will cover the basics in this course. If you feel like these concepts could be a bit more “present” in your head, you can go through the first section in the online book by Vasishth et al. (2022). There will be time to ask questions about the concepts in the sessions, but we will not have time to got through all of the concepts together. Hopefully, these resources will help to get everybody on the same page. If you need more support or materials, just let me know. Overview of dates Date Topic 28.10.2024 Simple and multiple regression 11.11.2024 Generalized linear models 25.11.2024 Pre/post measurements 09.12.2024 Longitudinal data 13.01.2025 Repeated measures 27.01.2025 Growth curve models 10.02.2025 Crossed random effects + Gelman &amp; Brown Topics for next semester: Multilevel models Simulation-based power analysis Power simulation LMM and GLMM Literature This course is mostly built on the following books and papers: Baayen et al. (2008) Bates et al. (2015) Gelman et al. (2020) Vasishth et al. (2022) Wickelmaier (2022) References Baayen, R. H., Davidson, D. J. &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/doi.org/10.1016/j.jml.2007.12.005 Bates, D., Mächler, M., Bolker, B. &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01 Gelman, A., Hill, J. &amp; Vehtari, A. (2020). Regression and other stories. Cambridge University Press. Vasishth, S., Schad, D., Bürki, A. &amp; Kliegl, R. (2022). Linear mixed models in linguistics and psychology: A comprehensive introduction. https://vasishth.github.io/Freq_CogSci Wickelmaier, F. (2022). Simulating the power of statistical tests: A collection of R examples. ArXiv. https://arxiv.org/abs/2110.09836 "],["simple-and-multiple-regression.html", "Chapter 1 Simple and multiple regression 1.1 Slides 1.2 Example to illustrate assumptions 1.3 Exercises", " Chapter 1 Simple and multiple regression This chapter is meant to give a short introduction to regression analysis. It is mostly meant to reintroduce some basic concepts, introduce notation, and get everybody on the same page. 1.1 Slides Unable to display PDF file. Download instead. 1.2 Example to illustrate assumptions Code data(anscombe) lm1 &lt;- lm(y1 ~ x1, anscombe) lm2 &lt;- lm(y2 ~ x2, anscombe) lm3 &lt;- lm(y3 ~ x3, anscombe) lm4 &lt;- lm(y4 ~ x4, anscombe) # Compare estimates rbind(coef(lm1), coef(lm2), coef(lm3), coef(lm4)) ## (Intercept) x1 ## [1,] 3.000091 0.5000909 ## [2,] 3.000909 0.5000000 ## [3,] 3.002455 0.4997273 ## [4,] 3.001727 0.4999091 Code # Plot data par(mfrow=c(2,2), mai = c(.6, .6, .1, .1), mgp = c(2.4, 1, 0)) plot(y1 ~ x1, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm1, lwd = 2) plot(y2 ~ x2, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm2, lwd = 2) plot(y3 ~ x3, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm3, lwd = 2) plot(y4 ~ x4, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm4, lwd = 2) Code # Look at assumption violations par(mfrow = c(2,2)) plot(lm1) Code plot(lm2) Code plot(lm3) Code plot(lm4) Code # Reshape data frame to long format dat &lt;- reshape(anscombe, direction = &quot;long&quot;, varying = list(paste0(&quot;x&quot;, 1:4), paste0(&quot;y&quot;, 1:4)), timevar = &quot;reg&quot;, v.names = c(&quot;x&quot;, &quot;y&quot;))[, -4] # drop id lattice::xyplot(y ~ x | as.factor(reg), dat, pch=16, type=c(&quot;p&quot;, &quot;r&quot;)) Code lattice::xyplot(y ~ x | as.factor(reg), dat, pch=16, type=c(&quot;p&quot;, &quot;smooth&quot;)) 1.3 Exercises Exercise 1 Simulate a data set based on a simple regression model with \\[\\begin{align*} \\beta_0 &amp; = 0.2\\\\ \\beta_1 &amp; = 0.3\\\\ \\sigma &amp; = 0.5\\\\ x &amp; \\in [1, 20]~\\text{in steps of 1} \\end{align*}\\] What functions in R do we need? Code x &lt;- 1:20 n &lt;- length(x) a &lt;- 0.2 b &lt;- 0.3 sigma &lt;- 0.5 y &lt;- 0.2 + 0.3*x + rnorm(n, sd = sigma) dat &lt;- data.frame(x, y) lm1 &lt;- lm(y ~ x, dat) summary(lm1) mean(resid(lm1)) sd(resid(lm1)) hist(resid(lm1), breaks = 15) Code # plot data plot(y ~ x, dat) abline(lm1) Exercise 2 Simulate data with the parameters from Exercise 1 Do not assume that we have one subject per value for \\(x\\), but more than one subject Simulate data for \\(n = 40\\) and \\(n = 100\\) Hint: Use sample(x, n, replace = TRUE) Re-cover your parameters as done on slide 11 What happens to your standard errors? Code n &lt;- 100 # 40 x0 &lt;- 1:20 x &lt;- sample(x0, n, replace = TRUE) a &lt;- 0.2 b &lt;- 0.3 sigma &lt;- 0.5 y &lt;- 0.2 + 0.3*x + rnorm(n, sd = sigma) dat &lt;- data.frame(x, y) pars &lt;- replicate(2000, { ysim &lt;- 0.2 + 0.3 * x + rnorm(n, sd = sigma) lm1 &lt;- lm(ysim ~ x, data = dat) c(coef(lm1), sigma(lm1)) }) rowMeans(pars) # standard errors apply(pars, 1, sd) hist(pars[1, ]) Code hist(pars[2, ]) Code hist(pars[3, ]) Code plot(y ~ jitter(x), dat) Exercise 3 Create two vectors \\(x\\) and \\(y\\) with 100 observations each and \\(X \\sim N(1,1)\\) and \\(Y \\sim N(2,1)\\). Create a data frame with variables id, group and score. \\(x\\) and \\(y\\) are your score values. Conduct a \\(t\\) test assuming that \\(X\\) and \\(Y\\) are independent having the same variances. Then use the function aov() to compute an analysis of variance for these data. Use then function lm() for a linear regression with predictor group and dependent variable score. Compare your results. Code x &lt;- rnorm(100, mean = 1) y &lt;- rnorm(100, mean = 2) dat &lt;- data.frame(id = 1:200, group = rep(c(&quot;x&quot;,&quot;y&quot;), each = 100), score = c(x, y)) rm(x,y) t1 &lt;- t.test(score ~ group, data = dat, var.equal = TRUE) lm1 &lt;- lm(score ~ group, data = dat) aov1 &lt;- aov(score ~ group, data = dat) (stat &lt;- list( coef = matrix(c(t1$estimate, lm1$coef, aov1$coef), nrow = 2, ncol = 3, dimnames = list(NULL, c(&quot;ttest&quot;, &quot;lm&quot;, &quot;aov&quot;))), statistics = matrix(c(t = t1$statistic^2, Flm = summary(lm1)$fstatistic[1], Faov = unlist(summary(aov1))[7]), nrow = 1, ncol = 3, dimnames = list(NULL, c(&quot;t&quot;,&quot;Flm&quot;,&quot;Faov&quot;)))) ) Exercise 4 The data set cars contains speed and stopping distances of 50 cars Estimate the regression model \\[ dist_i = \\beta_0 + \\beta_1 speed_i + \\varepsilon_i \\] How much variance of the stopping distances is explained by speed? Look at the residuals of the model. Are there any systematic deviances? Now estimate the model \\[ dist_i = \\beta_0 + \\beta_1 speed_i + \\beta_2 speed^2_i + \\varepsilon_i \\] Hint: Use I(speed^2) in the model formula Which model fits the data better? Code data(cars) lm1 &lt;- lm(dist ~ speed, data = cars) summary(lm1) hist(resid(lm1)) Code par(mfrow = c(2,2)) plot(lm1) Code lm2 &lt;- lm(dist ~ speed + I(speed^2), data = cars) anova(lm1, lm2) "],["generalized-linear-models.html", "Chapter 2 Generalized linear models 2.1 Slides 2.2 Exercises", " Chapter 2 Generalized linear models This chapter gives a hands-on introduction to generalized linear models with a focus on logistic regression for binary response data and poisson regression for count data. 2.1 Slides Unable to display PDF file. Download instead. 2.2 Exercises Exercise 1 In a psychophysical experiment two LEDs are presented to a subject: a standard with 40 cd/m\\(^2\\) and a comparison with varying intensities The subject is supposed to say which stimulus is brighter; each comparison is presented 40 times x (cd/m\\(^2\\)) 37 38 39 40 41 42 43 y (positiv) 2 3 10 25 34 36 39 Estimate parameters \\(c\\) and \\(a\\) of the logistic psychometric function \\[ p_{pos} = \\frac{1}{1 + \\exp(-\\frac{\\displaystyle x - c}{\\displaystyle a})} \\] using `glm()´ with \\(logit(p_{pos}) = \\beta_0 + \\beta_1x\\) where \\(a = 1/\\beta_1\\) and \\(c = -\\beta_0/\\beta_1\\). Calculate the intensity \\(x\\) for which \\(p_{pos} = 0.5\\) (Point of Subjective Equality, PSE) Create a plot for the probability to give a positive answer depending on the intensity of the comparison Use predict() to obtain the predicted values and add the logistic psychometric function to the plot Use abline() to add parameter \\(c\\) to the plot Use a likelihood ratio test to assess how well the model fits the data Is there reason to assume that there is any overdispersion? Code dat &lt;- data.frame(x = 37:43, y = c(2, 3, 10, 25, 34, 36, 39), n = 40) glm1 &lt;- glm(cbind(y, n - y) ~ x, family = binomial, data = dat) a &lt;- 1 / coef(glm1)[2] c &lt;- -coef(glm1)[1] / coef(glm1)[2] newx &lt;- seq(37, 43, .1) pre &lt;- predict(glm1, newdata = data.frame(x = newx), type = &quot;response&quot;) # Plot predictions with PSE plot(y/n ~ x, data = dat, pch = 16, ylab = &quot;Probability to say brighter&quot;) lines(pre ~ newx, data = dat) abline(v = c, h = .5, lty = 3) text(39, .8, paste(&quot;PSE =&quot;, round(c, 2))) Code # Goodness-of-fit test glms &lt;- glm(cbind(y, n - y) ~ factor(x), family = binomial, data = dat) anova(glm1, glms, test = &quot;Chisq&quot;) # Overdispersion summary(glm(cbind(y, n - y) ~ x, family = quasibinomial, data = dat)) Exercise 2 Fit a regression model to the Affairs data set from the AER package in R The variable affairs is the number of extramarital affairs in the past year and is our response variable Include the variables gender, age, yearsmarried, children, religiousness, education and rating as predictors religiousness ranges from 1 (anti) to 5 (very) and rating is a self rating of the marriage, ranging from 1 (very unhappy) to 5 (very happy) Assess the Goodness-of-fit using the deviance Assess overdispersion and decide if a model with an extra dispersion parameter might be indicated Compare the confidence intervals for the estimated parameters for both models Code # Load data set data(Affairs, package = &quot;AER&quot;) Affairs$rating &lt;- factor(Affairs$rating, levels = 1:5) ## Fit poisson model pois1 &lt;- glm(affairs ~ gender + age + yearsmarried + children + education + rating, family = poisson, data = Affairs) summary(pois1) # Goodness-of-fit 1 - pchisq(pois1$deviance, df = pois1$df.residual) ## Model with dispersion parameter pois2 &lt;- glm(affairs ~ gender + age + yearsmarried + children + education + rating, family = quasipoisson, data = Affairs) summary(pois2) ## Compare CIs confint(pois1) confint(pois2) Visualize model predictions Code ## Plot predictions for &quot;average&quot; females newdat &lt;- expand.grid(yearsmarried = seq(0, 15, 1), rating = factor(1:5)) newdat$gender &lt;- &quot;female&quot; newdat$age &lt;- mean(Affairs$age) newdat$children &lt;- &quot;yes&quot; newdat$religiousness &lt;- mean(Affairs$religiousness) newdat$education &lt;- mean(Affairs$education) newdat$pre &lt;- predict(pois2, newdata = newdat, type = &quot;response&quot;) colors &lt;- c(&quot;#78004B&quot;, &quot;#3CB4DC&quot;, &quot;#91C86E&quot;, &quot;#FF6900&quot;, &quot;#434F4F&quot;) plot(pre ~ yearsmarried, newdat, type = &quot;n&quot;, main = &quot;&#39;Average&#39; Females&quot;) for (i in 1:5) { lines(pre ~ yearsmarried, newdat[newdat$rating == i, ], col = colors[i]) } legend(&quot;topleft&quot;, c(&quot;very unhappy&quot;, &quot;somewhat unhappy&quot;, &quot;average&quot;, &quot;happier than average&quot;, &quot;very happy&quot;), col = colors, lty = 1, bty = &quot;n&quot;) "],["prepost-measurements.html", "Chapter 3 Pre/post measurements 3.1 Slides 3.2 Exercises", " Chapter 3 Pre/post measurements In this chapter, we start with the simplest form of longitudinal data: pre/post measurements. 3.1 Slides Unable to display PDF file. Download instead. 3.2 Exercises Exercise 1 Lowry (2000, Chapter 17) describes the following example Based on values \\(Y\\) of a finals exam, \\(m = 3\\) learning methods introducing basic programming skills will be compared (\\(i = 1, 2, 3\\)) In each of the chosen schools, 12 students attending grade 5 have been randomly selected (\\(j = 1, \\dots, 12\\)) for a six week course Before the course, familiarity with computers was assessed in a pre test as covariate \\(X\\) Data for method 1: pre 14 10 7 18 14 16 13 15 5 18 16 10 post 29 24 14 27 27 28 27 32 13 35 32 17 Data for method 2: pre 6 16 9 19 13 14 15 18 17 8 15 16 post 15 28 13 36 29 27 31 33 32 15 30 26 Data for method 3: pre 15 9 7 12 12 9 12 3 13 10 11 8 post 32 27 15 23 26 17 25 14 29 22 30 25 Fit the following models to data \\[\\begin{align} y_{ij} &amp;= \\beta_0 + \\beta_1 \\cdot x_{ij} + \\varepsilon_{ij}\\\\ y_{ij} &amp;= \\beta_0 + \\beta_1 \\cdot x_{ij} + \\beta_2 \\cdot z_{i} + \\varepsilon_{ij}\\\\ y_{ij} &amp;= \\beta_0 + \\beta_1 \\cdot x_{ij} + \\beta_2 \\cdot z_{i} + \\beta_3 \\cdot x_{ij} \\cdot z_i + \\varepsilon_{ij} \\end{align}\\] Plot the data with the predictions for each model Compare the models with a likelihood ratio test What are the adjusted means for the ANCOVA model? Code ## Fit models lm1 &lt;- lm(post ~ pre, data = dat) lm2 &lt;- lm(post ~ pre + method, data = dat) lm3 &lt;- lm(post ~ pre * method, data = dat) ## LRT anova(lm1, lm2, lm3) colors &lt;- c(&quot;#78004B&quot;, &quot;#FF6900&quot;, &quot;#3CB4DC&quot;) ## Plot model predictions par(mfrow = c(1, 3)) plot(post ~ pre, data = dat, col = colors[dat$method], pch = 16) abline(lm1) legend(&quot;topleft&quot;, paste(&quot;Method&quot;, 1:3), col = colors, pch = 16, bty = &quot;n&quot;) plot(post ~ pre, data = dat, col = colors[dat$method], pch = 16) abline(coef(lm2)[1], coef(lm2)[2], col = colors[1]) abline(coef(lm2)[1] + coef(lm2)[3], coef(lm2)[2], col = colors[2]) abline(coef(lm2)[1] + coef(lm2)[4], coef(lm2)[2], col = colors[3]) legend(&quot;topleft&quot;, paste(&quot;Method&quot;, 1:3), col = colors, pch = 16, bty = &quot;n&quot;) plot(post ~ pre, data = dat, col = colors[dat$method], pch = 16) abline(coef(lm3)[1], coef(lm3)[2], col = colors[1]) abline(coef(lm3)[1] + coef(lm3)[3], coef(lm3)[2] + coef(lm3)[5], col = colors[2]) abline(coef(lm3)[1] + coef(lm3)[4], coef(lm3)[2] + coef(lm3)[6], col = colors[3]) legend(&quot;topleft&quot;, paste(&quot;Method&quot;, 1:3), col = colors, pch = 16, bty = &quot;n&quot;) Code ## Means for pre and post measurements datagg &lt;- aggregate(cbind(pre, post) ~ method, data = dat, FUN = mean) ## Adjusted means datagg$post_adj &lt;- predict(lm2, newdata = data.frame(method = factor(1:3), pre = mean(dat$pre))) # Adjusted &quot;time effects&quot; datagg$diff &lt;- datagg$post - datagg$pre datagg$diff_adj &lt;- datagg$post_adj - mean(datagg$pre) Exercise 2 Simulate a data set for 75 subjects Create a factor condition indicating a control group and two treatment groups with 25 subjects each Simulate pre test data with \\(N \\sim (5, 1)\\) Simulate a post test score assuming a slope of 0.7 between pre and post score and that the first treatment group improves by 0.5 points compared to the control group and the second treatment group by 1 point What other assumption needs to be made? Code set.seed(1042) # set seed for reproducibility n &lt;- 75 condition &lt;- factor(rep(c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;), each = n / 3)) eff_cond &lt;- c(1, 0.5, 1) pre &lt;- rnorm(n, mean = 5, sd = 1) post &lt;- 0.7 * pre + model.matrix( ~ condition) %*% eff_cond + rnorm(n) dat &lt;- data.frame(id = factor(1:n), pre = pre, post = post, condition = condition ) rm(pre, post, condition) lattice::xyplot(post ~ pre, dat, groups = condition, type = c(&quot;g&quot;, &quot;p&quot;, &quot;r&quot;), auto.key = TRUE) Code # Observed averaged time effects, pre and post scores aggregate(cbind(post - pre, pre, post) ~ condition, dat, mean) Fit three different models to the data A Change Score model An ANCOVA model A mixed-effects model with predictors condition and time Compare the results of the three models What is the adjusted time effect for each group? Code library(lme4) ## Change Score Model #m1 &lt;- lm(post - pre ~ condition, dat) m1 &lt;- lm(post ~ condition + offset(pre), dat) summary(m1) ## ANCOVA Model m2 &lt;- lm(post ~ condition + pre, dat) summary(m2) # Adjusted means predict(m2, newdata = data.frame(condition = c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;), pre = mean(dat$pre))) ## Mixed Model (equivalent to change score model) dat_long &lt;- reshape(dat, direction = &quot;long&quot;, varying = list(c(&quot;pre&quot;, &quot;post&quot;)), v.names = &quot;resp&quot;, idvar = &quot;id&quot;, timevar = &quot;time&quot;, times = c(0, 1)) m3 &lt;- lmer(resp ~ condition*time + (1 | id), dat_long) summary(m3) ## Compare parameters of the three models cbind(coef(m1), coef(m2)[1:3], fixef(m3)[4:6]) datagg &lt;- aggregate(cbind(pre, post) ~ condition, data = dat, FUN = mean) ## Adjusted means datagg$post_adj &lt;- predict(m2, newdata = data.frame(condition = factor(c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;)), pre = mean(dat$pre))) # Adjusted &quot;time effects&quot; datagg$diff &lt;- datagg$post - datagg$pre datagg$diff_adj &lt;- datagg$post_adj - mean(datagg$pre) Exercise 3 Hedeker &amp; Gibbons (2006) report a smoking prevention study with students. The study used a \\(2 \\times 2\\) factorial design with factor “prevention curriculum” (with vs. without) and “TV prevention” (with vs. without). Students were randomly assigned to one of the four groups. Students’ knowledge about tabacco induced health risks before and after the prevention have been measured. Read the data set television.txt into R. Assign meaningful variable names. (You will find the necessary information at the beginning of the data file!) Calculate means and standard deviations for baseline, follow-up and change scores separately for the four groups. How many students are assigned to each group? Plot the follow-up means. Put intervention curriculum on the \\(y\\) axis and two seperate lines for each tv condition. Which effects do you expect based on this plot? Conduct an analysis for the follow-up scores (ANOVA), the change scores, the adjusted follow-up scores (ANCOVA), and the adjusted change score. Interpret the parameters for these models. How does the knowledge of the students change depending on the two factors? Assess how well the assumptions for the ANCOVA model hold using visual methods. Code dat &lt;- read.table(&quot;data/television.txt&quot;, skip = 43) names(dat) &lt;- c(&quot;school&quot;, &quot;class&quot;, &quot;curri&quot;, &quot;tv&quot;, &quot;pre&quot;, &quot;post&quot;) aggregate(cbind(pre, post, post - pre) ~ curri + tv, data = dat, FUN = mean) aggregate(cbind(pre, post) ~ curri + tv, data = dat, FUN = sd) aggregate(pre ~ curri + tv, data = dat, FUN = length) interaction.plot(dat$curri, dat$tv, dat$post, type = &quot;b&quot;, pch = c(1,16), xlab = &quot;Curriculum&quot;, ylab = &quot;Mean THKS score&quot;, trace.label = &quot;TV&quot;) Code summary(lm1 &lt;- lm(post ~ curri * tv, data = dat)) # Follow-Up summary(lm2 &lt;- lm(post - pre ~ curri * tv, data = dat)) # Change-Score summary(lm3 &lt;- lm(post ~ pre + curri * tv, data = dat)) # ANCOVA summary(lm4 &lt;- lm(post - pre ~ pre + curri * tv, data = dat)) # Change-Score ANCOVA plot(lm3) References Hedeker, D. R. &amp; Gibbons, R. D. (2006). Longitudinal data analysis. John Wiley. Lowry, R. (2000). Concepts and applications of inferential statistics. http://vassarstats.net/textbook/ "],["longitudinal-data.html", "Chapter 4 Longitudinal data 4.1 Slides 4.2 Exercises", " Chapter 4 Longitudinal data This chapter gives an introduction to linear mixed-effects models using a standard example: the sleepstudy data set. Random intercept and random slope models are introduced. 4.1 Slides Unable to display PDF file. Download instead. 4.2 Exercises Exercise 1 Load (or maybe install first) the package languageR. Then, load the data set quasif from this package. You can use ?quasif to inspect the data set. Use the function xtabs() to inspect the structure of your data. Are your factors crossed or nested? Create a box plot looking at the distribution of reaction time for short and long SOAs Visualize your data points individually for each subject. Use xyplot() from the lattice package or functions from the package ggplot2 Fit a model with random intercepts for Item and random intercepts as well as random slopes for Subject Test your random effects using likelihood-ratio tests Decide which model fits the data (empirically) best Compute confidence intervals for the estimated parameters of this model Look at the model assumptions for this model Code library(lattice) # load data set data(quasif, package = &quot;languageR&quot;) boxplot(RT ~ SOA, data = quasif) Code xtabs(~ SOA + Item, data = quasif) xtabs(~ Subject + Item, data = quasif) ftable(xtabs(~ Subject + Item + SOA, data = quasif)) xyplot(RT ~ SOA | Subject, data = quasif, groups = Item, auto.key = TRUE) Code lme1 &lt;- lmer(RT ~ SOA + (1 | Item) + (SOA | Subject), data = quasif, REML = FALSE) summary(lme1) lme2 &lt;- lmer(RT ~ SOA + (1 | Item) + (1 | Subject) + (1 | Subject:SOA), data = quasif, REML = FALSE) lme3 &lt;- lmer(RT ~ SOA + (1 | Item) + (1 | Subject), data = quasif, REML = FALSE) lme4 &lt;- lmer(RT ~ SOA + (1 | Subject), data = quasif, REML = FALSE) anova(lme4, lme3, lme2, lme1) confint(lme2, method = &quot;boot&quot;) plot(lme2, col = quasif$Subject, pch = quasif$Item) Code qqmath(lme2, col = quasif$Subject, pch = quasif$Item) "],["repeated-measurements.html", "Chapter 5 Repeated measurements 5.1 Slides 5.2 Exercises", " Chapter 5 Repeated measurements 5.1 Slides TODO 5.2 Exercises Code # TODO: Add exercises "],["data-simulation-for-linear-mixed-effects-models.html", "Chapter 6 Data simulation for linear mixed-effects models 6.1 Slides 6.2 Exercises", " Chapter 6 Data simulation for linear mixed-effects models 6.1 Slides TODO 6.2 Exercises Code # TODO: Add exercises "],["introduction-to-power.html", "Chapter 7 Introduction to power 7.1 Slides 7.2 Exercises", " Chapter 7 Introduction to power 7.1 Slides TODO 7.2 Exercises Code # TODO: Add exercises "],["power-simulation-for-glmm.html", "Chapter 8 Power simulation for GLMM 8.1 Slides 8.2 Exercises", " Chapter 8 Power simulation for GLMM 8.1 Slides TODO 8.2 Exercises Code # TODO: Add exercises "],["references.html", "References", " References Baayen, R. H., Davidson, D. J. &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/doi.org/10.1016/j.jml.2007.12.005 Bates, D., Mächler, M., Bolker, B. &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01 Gelman, A., Hill, J. &amp; Vehtari, A. (2020). Regression and other stories. Cambridge University Press. Hedeker, D. R. &amp; Gibbons, R. D. (2006). Longitudinal data analysis. John Wiley. Lowry, R. (2000). Concepts and applications of inferential statistics. http://vassarstats.net/textbook/ Vasishth, S., Schad, D., Bürki, A. &amp; Kliegl, R. (2022). Linear mixed models in linguistics and psychology: A comprehensive introduction. https://vasishth.github.io/Freq_CogSci Wickelmaier, F. (2022). Simulating the power of statistical tests: A collection of R examples. ArXiv. https://arxiv.org/abs/2110.09836 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
