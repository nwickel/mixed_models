[["index.html", "Course on Mixed-effects Models Course information Desirable background knowledge Overview of dates and topics Literature", " Course on Mixed-effects Models Nora Wickelmaier 2025-02-05 Course information This is an online book accompanying a course on mixed-effects models taught to PhD students at the Leibniz-Institut für Wissensmedien (IWM) in Tübingen. The data sets used in the examples and exercises can be downloaded here. So far, this is work in progress and very rudimentary! Desirable background knowledge Alle examples in this course are worked out in R. Introductory knowledge in R is therefore a prerequisite. If you want to freshen up your R knowledge a bit, you can go through the exercises provided here: https://nwickel.github.io/R_intro/. Apart from this, I will assume that you have some kind of workflow how to use R with a text editor (e.g., Vim) or an IDE (e.g., RStudio). It is further assumed that you have attended at least one introductory statistics course (level “Statisik I” and “Statistik II” usually taught in psychology curricula at German universities). That means you should be familiar with concepts like random variables, statistical distributions like the normal distribution, \\(t\\) distribution, Binomial distribution, etc., and hypothesis testing. Some background in regression analysis is helpful, but we will cover the basics in this course. If you feel like these concepts could be a bit more “present” in your head, you can go through the first chapter in the online book by Vasishth et al. (2022). There will be time to ask questions about the concepts in the sessions, but we will not have time to go through all of them together. Hopefully, these resources will help to get everybody on the same page. If you need more support or materials, just let me know. Overview of dates and topics Date Topic 28.10.2024 Simple and multiple regression 11.11.2024 Generalized linear models 25.11.2024 Pre/post measurements 09.12.2024 Longitudinal data 13.01.2025 Repeated measures 27.01.2025 Growth curve models 10.02.2025 Crossed random effects Topics for next semester: Multilevel models Simulation-based power analysis Power simulation LMM and GLMM Literature This course is mostly built on the following books and papers: Alday et al. (2025) Baayen et al. (2008) Bates et al. (2015) Gelman et al. (2020) Vasishth et al. (2022) Wickelmaier (2022) References Alday, P., Kliegl, R. &amp; Bates, D. (2025). Embrace uncertainty – Mixed-effects models with Julia. https://embraceuncertaintybook.com/ Baayen, R. H., Davidson, D. J. &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005 Bates, D., Mächler, M., Bolker, B. &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01 Gelman, A., Hill, J. &amp; Vehtari, A. (2020). Regression and other stories. Cambridge University Press. Vasishth, S., Schad, D., Bürki, A. &amp; Kliegl, R. (2022). Linear mixed models in linguistics and psychology: A comprehensive introduction. https://vasishth.github.io/Freq_CogSci Wickelmaier, F. (2022). Simulating the power of statistical tests: A collection of R examples. ArXiv. https://arxiv.org/abs/2110.09836 "],["simple-and-multiple-regression.html", "Chapter 1 Simple and multiple regression 1.1 Slides 1.2 Example to illustrate assumptions 1.3 Exercises", " Chapter 1 Simple and multiple regression This chapter is meant to give a short introduction to regression analysis. It is mostly meant to reintroduce some basic concepts, introduce notation, and get everybody on the same page. 1.1 Slides Unable to display PDF file. Download instead. 1.2 Example to illustrate assumptions Code data(anscombe) lm1 &lt;- lm(y1 ~ x1, anscombe) lm2 &lt;- lm(y2 ~ x2, anscombe) lm3 &lt;- lm(y3 ~ x3, anscombe) lm4 &lt;- lm(y4 ~ x4, anscombe) # Compare estimates rbind(coef(lm1), coef(lm2), coef(lm3), coef(lm4)) ## (Intercept) x1 ## [1,] 3.000091 0.5000909 ## [2,] 3.000909 0.5000000 ## [3,] 3.002455 0.4997273 ## [4,] 3.001727 0.4999091 Code # Plot data par(mfrow = c(2,2), mai = c(.6, .6, .1, .1), mgp = c(2.4, 1, 0)) plot(y1 ~ x1, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm1, lwd = 2) plot(y2 ~ x2, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm2, lwd = 2) plot(y3 ~ x3, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm3, lwd = 2) plot(y4 ~ x4, anscombe, pch = 16, col = &quot;blue&quot;) abline(lm4, lwd = 2) Code # Look at assumption violations par(mfrow = c(2,2)) plot(lm1) Code plot(lm2) Code plot(lm3) Code plot(lm4) Code # Reshape data frame to long format dat &lt;- reshape(anscombe, direction = &quot;long&quot;, varying = list(paste0(&quot;x&quot;, 1:4), paste0(&quot;y&quot;, 1:4)), timevar = &quot;reg&quot;, v.names = c(&quot;x&quot;, &quot;y&quot;))[, -4] # drop id lattice::xyplot(y ~ x | as.factor(reg), dat, pch = 16, type = c(&quot;p&quot;, &quot;r&quot;)) Code lattice::xyplot(y ~ x | as.factor(reg), dat, pch = 16, type = c(&quot;p&quot;, &quot;smooth&quot;)) 1.3 Exercises Exercise 1 Simulate a data set based on a simple regression model with \\[\\begin{align*} \\beta_0 &amp; = 0.2\\\\ \\beta_1 &amp; = 0.3\\\\ \\sigma &amp; = 0.5\\\\ x &amp; \\in [1, 20]~\\text{in steps of 1} \\end{align*}\\] What functions in R do we need? Code x &lt;- 1:20 n &lt;- length(x) a &lt;- 0.2 b &lt;- 0.3 sigma &lt;- 0.5 y &lt;- 0.2 + 0.3*x + rnorm(n, sd = sigma) dat &lt;- data.frame(x, y) lm1 &lt;- lm(y ~ x, dat) summary(lm1) mean(resid(lm1)) sd(resid(lm1)) hist(resid(lm1), breaks = 15) Code # plot data plot(y ~ x, dat) abline(lm1) Exercise 2 Simulate data with the parameters from Exercise 1 Do not assume that we have one subject per value for \\(x\\), but more than one subject Simulate data for \\(n = 40\\) and \\(n = 100\\) Hint: Use sample(x, n, replace = TRUE) Re-cover your parameters as done on slide 14 What happens to your standard errors? Code n &lt;- 100 # 40 x0 &lt;- 1:20 x &lt;- sample(x0, n, replace = TRUE) a &lt;- 0.2 b &lt;- 0.3 sigma &lt;- 0.5 y &lt;- 0.2 + 0.3*x + rnorm(n, sd = sigma) dat &lt;- data.frame(x, y) pars &lt;- replicate(2000, { ysim &lt;- 0.2 + 0.3 * x + rnorm(n, sd = sigma) lm1 &lt;- lm(ysim ~ x, data = dat) c(coef(lm1), sigma(lm1)) }) rowMeans(pars) # standard errors apply(pars, 1, sd) hist(pars[1, ]) Code hist(pars[2, ]) Code hist(pars[3, ]) Code plot(y ~ jitter(x), dat) Exercise 3 Create two vectors \\(x\\) and \\(y\\) with 100 observations each and \\(X \\sim N(1,1)\\) and \\(Y \\sim N(2,1)\\). Create a data frame with variables id, group and score. \\(x\\) and \\(y\\) are your score values. Conduct a \\(t\\) test assuming that \\(X\\) and \\(Y\\) are independent having the same variances. Then use the function aov() to compute an analysis of variance for these data. Use then function lm() for a linear regression with predictor group and dependent variable score. Compare your results. Code x &lt;- rnorm(100, mean = 1) y &lt;- rnorm(100, mean = 2) dat &lt;- data.frame(id = 1:200, group = rep(c(&quot;x&quot;,&quot;y&quot;), each = 100), score = c(x, y)) rm(x,y) t1 &lt;- t.test(score ~ group, data = dat, var.equal = TRUE) lm1 &lt;- lm(score ~ group, data = dat) aov1 &lt;- aov(score ~ group, data = dat) (stat &lt;- list( coef = matrix(c(t1$estimate, lm1$coef, aov1$coef), nrow = 2, ncol = 3, dimnames = list(NULL, c(&quot;ttest&quot;, &quot;lm&quot;, &quot;aov&quot;))), statistics = matrix(c(t = t1$statistic^2, Flm = summary(lm1)$fstatistic[1], Faov = unlist(summary(aov1))[7]), nrow = 1, ncol = 3, dimnames = list(NULL, c(&quot;t&quot;,&quot;Flm&quot;,&quot;Faov&quot;)))) ) Exercise 4 The data set cars contains speed and stopping distances of 50 cars Estimate the regression model \\[ dist_i = \\beta_0 + \\beta_1 speed_i + \\varepsilon_i \\] How much variance of the stopping distances is explained by speed? Look at the residuals of the model. Are there any systematic deviances? Now estimate the model \\[ dist_i = \\beta_0 + \\beta_1 speed_i + \\beta_2 speed^2_i + \\varepsilon_i \\] Hint: Use I(speed^2) in the model formula Which model fits the data better? Code data(cars) lm1 &lt;- lm(dist ~ speed, data = cars) summary(lm1) hist(resid(lm1)) Code par(mfrow = c(2,2)) plot(lm1) Code lm2 &lt;- lm(dist ~ speed + I(speed^2), data = cars) anova(lm1, lm2) "],["generalized-linear-models.html", "Chapter 2 Generalized linear models 2.1 Slides 2.2 Exercises", " Chapter 2 Generalized linear models This chapter gives a hands-on introduction to generalized linear models with a focus on logistic regression for binary response data and poisson regression for count data. 2.1 Slides Unable to display PDF file. Download instead. 2.2 Exercises Exercise 1 In a psychophysical experiment two LEDs are presented to a subject: a standard with 40 cd/m\\(^2\\) and a comparison with varying intensities The subject is supposed to say which stimulus is brighter; each comparison is presented 40 times x (cd/m\\(^2\\)) 37 38 39 40 41 42 43 y (positiv) 2 3 10 25 34 36 39 Estimate parameters \\(c\\) and \\(a\\) of the logistic psychometric function \\[ p_{pos} = \\frac{1}{1 + \\exp(-\\frac{\\displaystyle x - c}{\\displaystyle a})} \\] using glm() with \\(logit(p_{pos}) = \\beta_0 + \\beta_1x\\) where \\(a = 1/\\beta_1\\) and \\(c = -\\beta_0/\\beta_1\\). Calculate the intensity \\(x\\) for which \\(p_{pos} = 0.5\\) (Point of Subjective Equality, PSE) Create a plot for the probability to give a positive answer depending on the intensity of the comparison Use predict() to obtain the predicted values and add the logistic psychometric function to the plot Use abline() to add parameter \\(c\\) to the plot Use a likelihood ratio test to assess how well the model fits the data Is there reason to assume that there is any overdispersion? Code dat &lt;- data.frame(x = 37:43, y = c(2, 3, 10, 25, 34, 36, 39), n = 40) glm1 &lt;- glm(cbind(y, n - y) ~ x, family = binomial, data = dat) a &lt;- 1 / coef(glm1)[2] c &lt;- -coef(glm1)[1] / coef(glm1)[2] newx &lt;- seq(37, 43, .1) pre &lt;- predict(glm1, newdata = data.frame(x = newx), type = &quot;response&quot;) # Plot predictions with PSE plot(y/n ~ x, data = dat, pch = 16, ylab = &quot;Probability to say brighter&quot;) lines(pre ~ newx, data = dat) abline(v = c, h = .5, lty = 3) text(39, .8, paste(&quot;PSE =&quot;, round(c, 2))) Code # Goodness-of-fit test glms &lt;- glm(cbind(y, n - y) ~ factor(x), family = binomial, data = dat) anova(glm1, glms, test = &quot;Chisq&quot;) # Overdispersion summary(glm(cbind(y, n - y) ~ x, family = quasibinomial, data = dat)) Exercise 2 Fit a regression model to the Affairs data set from the AER package in R The variable affairs is the number of extramarital affairs in the past year and is our response variable Include the variables gender, age, yearsmarried, children, religiousness, education and rating as predictors religiousness ranges from 1 (anti) to 5 (very) and rating is a self rating of the marriage, ranging from 1 (very unhappy) to 5 (very happy) Assess the Goodness-of-fit using the deviance Assess overdispersion and decide if a model with an extra dispersion parameter might be indicated Compare the confidence intervals for the estimated parameters for both models Code # Load data set data(Affairs, package = &quot;AER&quot;) Affairs$rating &lt;- factor(Affairs$rating, levels = 1:5) ## Fit poisson model pois1 &lt;- glm(affairs ~ gender + age + yearsmarried + children + education + rating, family = poisson, data = Affairs) summary(pois1) # Goodness-of-fit 1 - pchisq(pois1$deviance, df = pois1$df.residual) ## Model with dispersion parameter pois2 &lt;- glm(affairs ~ gender + age + yearsmarried + children + education + rating, family = quasipoisson, data = Affairs) summary(pois2) ## Compare CIs confint(pois1) confint(pois2) Visualize model predictions Code ## Plot predictions for &quot;average&quot; females newdat &lt;- expand.grid(yearsmarried = seq(0, 15, 1), rating = factor(1:5)) newdat$gender &lt;- &quot;female&quot; newdat$age &lt;- mean(Affairs$age) newdat$children &lt;- &quot;yes&quot; newdat$religiousness &lt;- mean(Affairs$religiousness) newdat$education &lt;- mean(Affairs$education) newdat$pre &lt;- predict(pois2, newdata = newdat, type = &quot;response&quot;) colors &lt;- c(&quot;#78004B&quot;, &quot;#3CB4DC&quot;, &quot;#91C86E&quot;, &quot;#FF6900&quot;, &quot;#434F4F&quot;) plot(pre ~ yearsmarried, newdat, type = &quot;n&quot;, main = &quot;&#39;Average&#39; Females&quot;) for (i in 1:5) { lines(pre ~ yearsmarried, newdat[newdat$rating == i, ], col = colors[i]) } legend(&quot;topleft&quot;, c(&quot;very unhappy&quot;, &quot;somewhat unhappy&quot;, &quot;average&quot;, &quot;happier than average&quot;, &quot;very happy&quot;), col = colors, lty = 1, bty = &quot;n&quot;) "],["prepost-measurements.html", "Chapter 3 Pre/post measurements 3.1 Slides 3.2 Exercises", " Chapter 3 Pre/post measurements In this chapter, we start with the simplest form of longitudinal data: pre/post measurements. 3.1 Slides Unable to display PDF file. Download instead. 3.2 Exercises Exercise 1 Lowry (2000, Chapter 17) describes the following example Based on values \\(Y\\) of a finals exam, \\(m = 3\\) learning methods introducing basic programming skills will be compared (\\(i = 1, 2, 3\\)) In each of the chosen schools, 12 students attending grade 5 have been randomly selected (\\(j = 1, \\dots, 12\\)) for a six week course Before the course, familiarity with computers was assessed in a pre test as covariate \\(X\\) Data for method 1: pre 14 10 7 18 14 16 13 15 5 18 16 10 post 29 24 14 27 27 28 27 32 13 35 32 17 Data for method 2: pre 6 16 9 19 13 14 15 18 17 8 15 16 post 15 28 13 36 29 27 31 33 32 15 30 26 Data for method 3: pre 15 9 7 12 12 9 12 3 13 10 11 8 post 32 27 15 23 26 17 25 14 29 22 30 25 Fit the following models to the data \\[\\begin{align} y_{ij} &amp;= \\beta_0 + \\beta_1 \\cdot x_{ij} + \\varepsilon_{ij}\\\\ y_{ij} &amp;= \\beta_0 + \\beta_1 \\cdot x_{ij} + \\beta_2 \\cdot z_{i} + \\varepsilon_{ij}\\\\ y_{ij} &amp;= \\beta_0 + \\beta_1 \\cdot x_{ij} + \\beta_2 \\cdot z_{i} + \\beta_3 \\cdot x_{ij} \\cdot z_i + \\varepsilon_{ij} \\end{align}\\] Plot the data with the predictions for each model Compare the models with a likelihood ratio test What are the adjusted means for the ANCOVA model? Code ## Fit models lm1 &lt;- lm(post ~ pre, data = dat) lm2 &lt;- lm(post ~ pre + method, data = dat) lm3 &lt;- lm(post ~ pre * method, data = dat) ## LRT anova(lm1, lm2, lm3) colors &lt;- c(&quot;#78004B&quot;, &quot;#FF6900&quot;, &quot;#3CB4DC&quot;) ## Plot model predictions par(mfrow = c(1, 3)) plot(post ~ pre, data = dat, col = colors[dat$method], pch = 16) abline(lm1) legend(&quot;topleft&quot;, paste(&quot;Method&quot;, 1:3), col = colors, pch = 16, bty = &quot;n&quot;) plot(post ~ pre, data = dat, col = colors[dat$method], pch = 16) abline(coef(lm2)[1], coef(lm2)[2], col = colors[1]) abline(coef(lm2)[1] + coef(lm2)[3], coef(lm2)[2], col = colors[2]) abline(coef(lm2)[1] + coef(lm2)[4], coef(lm2)[2], col = colors[3]) legend(&quot;topleft&quot;, paste(&quot;Method&quot;, 1:3), col = colors, pch = 16, bty = &quot;n&quot;) plot(post ~ pre, data = dat, col = colors[dat$method], pch = 16) abline(coef(lm3)[1], coef(lm3)[2], col = colors[1]) abline(coef(lm3)[1] + coef(lm3)[3], coef(lm3)[2] + coef(lm3)[5], col = colors[2]) abline(coef(lm3)[1] + coef(lm3)[4], coef(lm3)[2] + coef(lm3)[6], col = colors[3]) legend(&quot;topleft&quot;, paste(&quot;Method&quot;, 1:3), col = colors, pch = 16, bty = &quot;n&quot;) Code ## Means for pre and post measurements datagg &lt;- aggregate(cbind(pre, post) ~ method, data = dat, FUN = mean) ## Adjusted means datagg$post_adj &lt;- predict(lm2, newdata = data.frame(method = factor(1:3), pre = mean(dat$pre))) # Adjusted &quot;time effects&quot; datagg$diff &lt;- datagg$post - datagg$pre datagg$diff_adj &lt;- datagg$post_adj - mean(datagg$pre) Exercise 2 Simulate a data set for 75 subjects Create a factor condition indicating a control group and two treatment groups with 25 subjects each Simulate pre test data with \\(N \\sim (5, 1)\\) Simulate a post test score assuming a slope of 0.7 between pre and post score and that the first treatment group improves by 0.5 points compared to the control group and the second treatment group by 1 point What other assumption needs to be made? Code set.seed(1042) # set seed for reproducibility n &lt;- 75 condition &lt;- factor(rep(c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;), each = n / 3)) eff_cond &lt;- c(1, 0.5, 1) pre &lt;- rnorm(n, mean = 5, sd = 1) post &lt;- 0.7 * pre + model.matrix( ~ condition) %*% eff_cond + rnorm(n) dat &lt;- data.frame(id = factor(1:n), pre = pre, post = post, condition = condition ) rm(pre, post, condition) lattice::xyplot(post ~ pre, dat, groups = condition, type = c(&quot;g&quot;, &quot;p&quot;, &quot;r&quot;), auto.key = TRUE) Code # Observed averaged time effects, pre and post scores aggregate(cbind(post - pre, pre, post) ~ condition, dat, mean) Fit three different models to the data A Change Score model An ANCOVA model A mixed-effects model with predictors condition and time Compare the results of the three models What is the adjusted time effect for each group? Code library(lme4) ## Change Score Model #m1 &lt;- lm(post - pre ~ condition, dat) m1 &lt;- lm(post ~ condition + offset(pre), dat) summary(m1) ## ANCOVA Model m2 &lt;- lm(post ~ condition + pre, dat) summary(m2) # Adjusted means predict(m2, newdata = data.frame(condition = c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;), pre = mean(dat$pre))) ## Mixed Model (equivalent to change score model) dat_long &lt;- reshape(dat, direction = &quot;long&quot;, varying = list(c(&quot;pre&quot;, &quot;post&quot;)), v.names = &quot;resp&quot;, idvar = &quot;id&quot;, timevar = &quot;time&quot;, times = c(0, 1)) m3 &lt;- lmer(resp ~ condition * time + (1 | id), dat_long) summary(m3) ## Compare parameters of the three models cbind(coef(m1), coef(m2)[1:3], fixef(m3)[4:6]) datagg &lt;- aggregate(cbind(pre, post) ~ condition, data = dat, FUN = mean) ## Adjusted means datagg$post_adj &lt;- predict(m2, newdata = data.frame(condition = factor(c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;)), pre = mean(dat$pre))) # Adjusted &quot;time effects&quot; datagg$diff &lt;- datagg$post - datagg$pre datagg$diff_adj &lt;- datagg$post_adj - mean(datagg$pre) # OR m4 &lt;- lm(post - pre ~ condition + scale(pre), dat) summary(m4) # Adjusted &quot;time effects&quot; predict(m4, newdata = data.frame(condition = c(&quot;contr&quot;, &quot;treat1&quot;, &quot;treat2&quot;), pre = mean(dat$pre))) Exercise 3 Hedeker &amp; Gibbons (2006) report a smoking prevention study with students. The study used a \\(2 \\times 2\\) factorial design with factor “prevention curriculum” (with vs. without) and “TV prevention” (with vs. without). Students were randomly assigned to one of the four groups. Students’ knowledge about tabacco induced health risks before and after the prevention have been measured. Read the data set television.txt into R. Assign meaningful variable names. (You will find the necessary information at the beginning of the data file!) Calculate means and standard deviations for baseline, follow-up and change scores separately for the four groups. How many students are assigned to each group? Plot the follow-up means. Put intervention curriculum on the \\(y\\) axis and two seperate lines for each tv condition. Which effects do you expect based on this plot? Conduct an analysis for the follow-up scores (ANOVA), the change scores, the adjusted follow-up scores (ANCOVA), and the adjusted change score. Interpret the parameters for these models. How does the knowledge of the students change depending on the two factors? Assess how well the assumptions for the ANCOVA model hold using visual methods. Code dat &lt;- read.table(&quot;data/television.txt&quot;, skip = 43) names(dat) &lt;- c(&quot;school&quot;, &quot;class&quot;, &quot;curri&quot;, &quot;tv&quot;, &quot;pre&quot;, &quot;post&quot;) aggregate(cbind(pre, post, post - pre) ~ curri + tv, data = dat, FUN = mean) aggregate(cbind(pre, post) ~ curri + tv, data = dat, FUN = sd) aggregate(pre ~ curri + tv, data = dat, FUN = length) interaction.plot(dat$curri, dat$tv, dat$post, type = &quot;b&quot;, pch = c(1,16), xlab = &quot;Curriculum&quot;, ylab = &quot;Mean THKS score&quot;, trace.label = &quot;TV&quot;) Code summary(lm1 &lt;- lm(post ~ curri * tv, data = dat)) # Follow-Up summary(lm2 &lt;- lm(post - pre ~ curri * tv, data = dat)) # Change-Score summary(lm3 &lt;- lm(post ~ pre + curri * tv, data = dat)) # ANCOVA summary(lm4 &lt;- lm(post - pre ~ pre + curri * tv, data = dat)) # Change-Score ANCOVA plot(lm3) References Hedeker, D. R. &amp; Gibbons, R. D. (2006). Longitudinal data analysis. John Wiley. Lowry, R. (2000). Concepts and applications of inferential statistics. http://vassarstats.net/textbook/ "],["longitudinal-data.html", "Chapter 4 Longitudinal data 4.1 Slides 4.2 Exercises", " Chapter 4 Longitudinal data This chapter gives an introduction to linear mixed-effects models using a standard example: the sleepstudy data set. Random intercept and random slope models are introduced. 4.1 Slides Unable to display PDF file. Download instead. 4.2 Exercises Exercise 1 Load (or maybe install first) the package languageR. Then, load the data set quasif from this package. You can use ?quasif to inspect the data set. Use the function xtabs() to inspect the structure of your data. Are your factors crossed or nested? Create a box plot looking at the distribution of reaction time for short and long SOAs Visualize your data points individually for each subject. Use xyplot() from the lattice package or functions from the package ggplot2 Fit a model with random intercepts for Item and random intercepts as well as random slopes for Subject Test your random effects using likelihood-ratio tests Decide which model fits the data (empirically) best Compute confidence intervals for the estimated parameters of this model Look at the model assumptions for this model Code library(lme4) library(lattice) # load data set data(quasif, package = &quot;languageR&quot;) boxplot(RT ~ SOA, data = quasif) Code xtabs(~ SOA + Item, data = quasif) xtabs(~ Subject + Item, data = quasif) ftable(xtabs(~ Subject + Item + SOA, data = quasif)) xyplot(RT ~ SOA | Subject, data = quasif, groups = Item, auto.key = TRUE) Code lme1 &lt;- lmer(RT ~ 1 + SOA + (1 | Item) + (1 + SOA | Subject), data = quasif, REML = FALSE) summary(lme1) lme2 &lt;- lmer(RT ~ 1 + SOA + (1 | Item) + (1 | Subject), data = quasif, REML = FALSE) lme3 &lt;- lmer(RT ~ 1 + SOA + (1 + SOA | Subject), data = quasif, REML = FALSE) lme4 &lt;- lmer(RT ~ 1 + SOA + (1 | Subject), data = quasif, REML = FALSE) anova(lme4, lme2, lme1) anova(lme4, lme3, lme1) confint(lme1, method = &quot;boot&quot;) plot(lme1, col = quasif$Subject, pch = quasif$Item) Code qqmath(lme1, col = quasif$Subject, pch = quasif$Item) "],["repeated-measurements.html", "Chapter 5 Repeated measurements 5.1 Slides 5.2 Exercises", " Chapter 5 Repeated measurements In this chapter, we will go on with models for longitudinal data and compare mixed-effects models to repeated measures ANOVA. 5.1 Slides Unable to display PDF file. Download instead. 5.2 Exercises Exercise 1 Expand the linear mixed-effects model to more than one factor Add diagnosis (“endogenous” vs. “non-endogenous”) as additional between factor Test if this factor interacts with week using a likelihood-ratio test Use parametric bootstrapping to get a sampling distribution for your LRT statistic Plot the sampling distribution and add the empirical confidence interval Plot the predictions of the model \\[ y_{ij} = \\beta_0 + \\beta_1 time + \\beta_2 diag + \\upsilon_{0i} + \\upsilon_{1i} time + \\varepsilon_{ij} \\] with \\[\\begin{align*} \\begin{pmatrix} \\upsilon_{0i}\\\\ \\upsilon_{1i} \\end{pmatrix} &amp;\\sim N \\left(\\begin{pmatrix} 0\\\\ 0 \\end{pmatrix}, \\, \\boldsymbol{\\Sigma}_\\upsilon = \\begin{pmatrix} \\sigma^2_{\\upsilon_0} &amp; \\sigma_{\\upsilon_0 \\upsilon_1} \\\\ \\sigma_{\\upsilon_0 \\upsilon_1} &amp; \\sigma^2_{\\upsilon_1} \\\\ \\end{pmatrix} \\right) \\text{ i.i.d.} \\\\ \\boldsymbol{\\varepsilon}_i &amp;\\sim N(\\mathbf{0}, \\, \\sigma^2 \\mathbf{I}_{n_i}) \\text{ i.i.d.} \\end{align*}\\] Code library(lme4) dat &lt;- read.table(&quot;data/reisby.dat&quot;, header = TRUE) dat$id &lt;- factor(dat$id) dat$diag &lt;- factor(dat$diag, levels = c(&quot;nonen&quot;, &quot;endog&quot;)) dat &lt;- na.omit(dat) # drop missing values # Add diagnosis lme1 &lt;- lmer(hamd ~ week + diag + (week | id), dat, REML = FALSE) lme2 &lt;- lmer(hamd ~ week * diag + (week | id), dat, REML = FALSE) anova(lme1, lme2) Code # Parametric bootstrapping nsim &lt;- 1000 LR &lt;- rep(NA, nsim) sim2 &lt;- simulate(lme2, nsim) for (i in seq_len(nsim)) { fit1 &lt;- lmer(sim2[, i] ~ week + diag + (week | id), dat, REML = FALSE) fit2 &lt;- lmer(sim2[, i] ~ week * diag + (week | id), dat, REML = FALSE) LR[i] &lt;- 2 * (summary(fit2)$logLik - summary(fit1)$logLik) } hist(LR, breaks = 50, col = &quot;grey&quot;, border = &quot;white&quot;, freq = FALSE, main = &quot;&quot;) curve(dchisq(x, df = 1), add = TRUE) abline(v = 2 * (summary(lme2)$logLik - summary(lme1)$logLik), lty = 2) Code # Empirical p-value mean(LR &gt; 2 * (summary(lme2)$logLik - summary(lme1)$logLik)) Code # Predictions with diagnosis as fixed effect datm &lt;- aggregate(hamd ~ week + diag, data = dat, FUN = mean) datm$pred1 &lt;- predict(lme1, newdata = datm, re.form = ~0) colors &lt;- c(&quot;#3CB4DC&quot;, &quot;#91C86E&quot;) plot(hamd ~ week, data = datm, col = colors[datm$diag], ylim = c(0, 28), xlab = &quot;Week&quot;, ylab = &quot;HDRS score&quot;) lines(pred1 ~ week, data = subset(datm, datm$diag == &quot;nonen&quot;), col = colors[1]) lines(pred1 ~ week, data = subset(datm, datm$diag == &quot;endog&quot;), col = colors[2]) legend(&quot;bottomleft&quot;, legend = c(&quot;Non endogenous&quot;, &quot;Endogenous&quot;), col = colors, pch = 21, lty = 1, pt.bg = &quot;white&quot;, bty = &quot;n&quot;) "],["growth-curve-models.html", "Chapter 6 Growth curve models 6.1 Slides 6.2 Exercises", " Chapter 6 Growth curve models In this chapter, we extend the models we have looked at so far and include quadratic effects for individuals and on the population level. 6.1 Slides Unable to display PDF file. Download instead. 6.2 Exercises Exercise 1 The following exercise is strongly inspired by this chapter: https://embraceuncertaintybook.com/longitudinal.html#the-elstongrizzle-data. Load the data set into R; data are from a dental study measuring the lengths of the ramus of the mandible (mm) in 20 boys at 8, 8.5, 9, and 9.5 years of age Plot the individual data points for each subject either as a spaghetti plot and/or as a panel plot Fit a random slope model to the data; how would you interpret the correlation parameter in the model? Recenter your time variable, so that zero means ``8 years old’’ Refit your random slope model; try to explain why and how the correlation parameter changes Look at the caterpillar plots for the random slope model with and without recentered time variable; why do they look different? Create a shrinkage plot plotting the individual intercept as a function of the individual slopes Add individual and quadratic time effects to your model; test this model against the random slope model Look at the profiles for the random effects for the quadratic model; what would you conclude? Code library(lme4) library(lattice) # Read and visualize data dat &lt;- read.table(&quot;data/elstongrizzle.dat&quot;, header = TRUE) dat$subject &lt;- factor(dat$subject) xyplot(ramusht ~ age, dat, groups = subject, type = c(&quot;b&quot;, &quot;g&quot;), xlab = &quot;Age (yr)&quot;, ylab = &quot;Ramus bone length (mm)&quot;) Code xyplot(ramusht ~ age | subject, dat, type = c(&quot;g&quot;, &quot;p&quot;, &quot;r&quot;), aspect = &quot;xy&quot;, index.cond = function(x,y) coef(lm(y ~ x)) %*% c(1, 8), xlab = &quot;Age (yr)&quot;, ylab = &quot;Ramus bone length (mm)&quot;) Code # Anything quadratic? xyplot(ramusht ~ age | subject, dat, type = c(&quot;g&quot;, &quot;p&quot;, &quot;spline&quot;), aspect = &quot;xy&quot;, index.cond = function(x,y) coef(lm(y ~ x)) %*% c(1, 8), xlab = &quot;Age (yr)&quot;, ylab = &quot;Ramus bone length (mm)&quot;) Code # Random slope model m1 &lt;- lmer(ramusht ~ age + (age | subject), dat) summary(m1) dotplot(ranef(m1), scales = list(x = list(relation = &quot;free&quot;)))[[1]] Code # Centering the time variable (at a value of interest) dat$time &lt;- dat$age - 8 m2 &lt;- lmer(ramusht ~ time + (time | subject), dat) summary(m2) dotplot(ranef(m2), scales = list(x = list(relation = &quot;free&quot;)))[[1]] Code # Shrinkage plot df &lt;- coef(lmList(ramusht ~ time | subject, dat)) fclow &lt;- subset(df, `(Intercept)` &lt; 50) fchigh &lt;- subset(df, `(Intercept)` &gt; 50) cc1 &lt;- as.data.frame(coef(m2)$subject) names(cc1) &lt;- c(&quot;A&quot;, &quot;B&quot;) df &lt;- cbind(df, cc1) ff &lt;- fixef(m2) with(df, xyplot(`(Intercept)` ~ time, aspect = 1, x1 = B, y1 = A, panel = function(x, y, x1, y1, subscripts, ...) { panel.grid(h = -1, v = -1) x1 &lt;- x1[subscripts] y1 &lt;- y1[subscripts] larrows(x, y, x1, y1, type = &quot;closed&quot;, length = 0.1, angle = 15, ...) lpoints(x, y, pch = trellis.par.get(&quot;superpose.symbol&quot;)$pch[2], col = trellis.par.get(&quot;superpose.symbol&quot;)$col[2]) lpoints(x1, y1, pch = trellis.par.get(&quot;superpose.symbol&quot;)$pch[1], col = trellis.par.get(&quot;superpose.symbol&quot;)$col[1]) lpoints(ff[2], ff[1], pch = trellis.par.get(&quot;superpose.symbol&quot;)$pch[3], col = trellis.par.get(&quot;superpose.symbol&quot;)$col[3]) ltext(fclow[,2], fclow[,1], row.names(fclow), adj = c(0.5, 1.7)) ltext(fchigh[,2], fchigh[,1], row.names(fchigh), adj = c(0.5, -0.6)) }, key = list(space = &quot;top&quot;, columns = 3, text = list(c(&quot;Mixed model&quot;, &quot;Within-subject&quot;, &quot;Population&quot;)), points = list(col = trellis.par.get(&quot;superpose.symbol&quot;)$col[1:3], pch = trellis.par.get(&quot;superpose.symbol&quot;)$pch[1:3])) ) ) Code # Fitting a quadratic trend m3 &lt;- lmer(ramusht ~ time + I(time^2) + (time + I(time^2) || subject), dat) summary(m3) dotplot(ranef(m3), scales = list(x = list(relation = &quot;free&quot;)))[[1]] Code anova(m2, m3) # Profiles pm3 &lt;- profile(m3, which = &quot;theta_&quot;) xyplot(log(pm3)) Code densityplot(log(pm3)) Code splom(log(pm3)) Code confint(pm3) "],["crossed-random-effects.html", "Chapter 7 Crossed random effects 7.1 Slides 7.2 Exercises", " Chapter 7 Crossed random effects 7.1 Slides Unable to display PDF file. Download instead. 7.2 Exercises Exercise 1 Change the data simulation by Baayen et al. (2008) for \\(N = 30\\) subjects instead of only 3 You can use the following script and adjust it accordingly You can choose if you want to use model matrices or create the vectors “manually” Code library(lattice) library(lme4) #--------------- (1) Create data frame --------------- datsim &lt;- expand.grid(subject = factor(c(&quot;s1&quot; , &quot;s2&quot; , &quot;s3&quot; )), item = factor(c(&quot;w1&quot; , &quot;w2&quot; , &quot;w3&quot; )), soa = factor(c(&quot;long&quot; , &quot;short&quot; ))) datsim &lt;- datsim |&gt; sort_by(~ subject) #--------------- (2) Define parameters --------------- beta0 &lt;- 522.11 beta1 &lt;- -18.89 sw &lt;- 21.1 sy0 &lt;- 23.89 sy1 &lt;- 9 ry &lt;- -1 se &lt;- 9.9 #--------------- (3) Create vectors and simulate data --------------- # Fixed effects b0 &lt;- rep(beta0, 18) b1 &lt;- rep(rep(c(0, beta1), each = 3), 3) # Draw random effects w &lt;- rep(rnorm(3, mean = 0, sd = sw), 6) e &lt;- rnorm(18, mean = 0, sd = se) # Bivariate normal distribution sig &lt;- matrix(c(sy0^2, ry * sy0 * sy1, ry * sy0 * sy1, sy1^2), 2, 2) y01 &lt;- mvtnorm::rmvnorm(3, mean = c(0, 0), sigma = sig) y0 &lt;- rep(y01[,1], each = 6) y1 &lt;- rep(c(0, y01[1,2], 0, y01[2,2], 0, y01[3,2]), each = 3) datsim$rt &lt;- b0 + b1 + w + y0 + y1 + e #--------------- (4) Simulate data using model matrices --------------- X &lt;- model.matrix( ~ soa, datsim) Z &lt;- model.matrix( ~ 0 + item + subject + subject:soa, datsim, contrasts.arg = list(subject = contrasts(datsim$subject, contrasts = FALSE))) # Fixed effects beta &lt;- c(beta0, beta1) # Random effects u &lt;- c(w = unique(w), y0 = y01[,1], y1 = y01[,2]) datsim$rt2 &lt;- X %*% beta + Z %*% u + e #--------------- (5) Visualize simulated data --------------- xyplot(rt ~ soa | subject, datsim, group = item, type = &quot;b&quot;, layout = c(3, 1)) Code n &lt;- 30 datsim &lt;- expand.grid(subject = factor(paste0(&quot;s&quot;, 1:n)), item = factor(c(&quot;w1&quot; , &quot;w2&quot; , &quot;w3&quot; )), soa = factor(c(&quot;long&quot; , &quot;short&quot; ))) datsim &lt;- datsim |&gt; sort_by(~ subject) beta0 &lt;- 522.11 beta1 &lt;- -18.89 sw &lt;- 21.1 sy0 &lt;- 23.89 sy1 &lt;- 9 ry &lt;- -1 se &lt;- 9.9 w &lt;- rnorm(3, mean = 0, sd = sw) e &lt;- rnorm(n * 6, mean = 0, sd = se) # Bivariate normal distribution sig &lt;- matrix(c(sy0^2, ry * sy0 * sy1, ry * sy0 * sy1, sy1^2), 2, 2) y01 &lt;- mvtnorm::rmvnorm(n, mean = c(0, 0), sigma = sig) beta &lt;- c(beta0, beta1) # Random effects u &lt;- c(w = w, y0 = y01[,1], y1 = y01[,2]) X &lt;- model.matrix( ~ soa, datsim) Z &lt;- model.matrix( ~ 0 + item + subject + subject:soa, datsim, contrasts.arg = list(subject = contrasts(datsim$subject, contrasts = FALSE))) datsim$rt &lt;- X %*% beta + Z %*% u + e xyplot(rt ~ soa | subject, datsim, group = item, type = &quot;b&quot;, layout=c(5, 6)) Exercise 2 Fit the following models to the healing data by Aungle &amp; Langer (2023) Code load(&quot;data/healing.RData&quot;) dat &lt;- DFmodel m1 &lt;- lmer(Healing ~ Condition + (1 | Subject) + (1 | ResponseId), dat) m2 &lt;- lmer(Healing ~ Condition + (Condition | Subject) + (1 | ResponseId), dat) m3 &lt;- lmer(Healing ~ Condition + (1 | Subject) + (0 + dummy(Condition, &quot;28&quot;) | Subject) + (0 + dummy(Condition, &quot;56&quot;) | Subject) + (1 | ResponseId), dat) m4 &lt;- lmer(Healing ~ Condition + (Condition | Subject) + (Condition | ResponseId), dat) Profile the models with profile(&lt;model&gt;) Use the functions xyplot(), densityplot(), splom() from the lattice package to take a closer look at the estimated random parameters Compare the three models with likelihood ratio tests What is the best model in your opinion? Code pm1 &lt;- profile(m1) pm2 &lt;- profile(m2) pm3 &lt;- profile(m3) xyplot(pm1, which = &quot;theta_&quot;) Code xyplot(pm2, which = &quot;theta_&quot;) Code xyplot(pm2, which = &quot;theta_&quot;) Code densityplot(pm1, which = &quot;theta_&quot;) Code densityplot(pm2, which = &quot;theta_&quot;) Code densityplot(pm3, which = &quot;theta_&quot;) Code splom(pm1, which = &quot;theta_&quot;) Code splom(pm2, which = &quot;theta_&quot;) Code splom(pm3, which = &quot;theta_&quot;) Code anova(m1, m3, m2) References Aungle, P. &amp; Langer, E. (2023). Physical healing as a function of perceived time. Scientific Reports, 13(1), 22432. https://doi.org/10.1038/s41598-023-50009-3 Baayen, R. H., Davidson, D. J. &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005 "],["references.html", "References", " References Alday, P., Kliegl, R. &amp; Bates, D. (2025). Embrace uncertainty – Mixed-effects models with Julia. https://embraceuncertaintybook.com/ Aungle, P. &amp; Langer, E. (2023). Physical healing as a function of perceived time. Scientific Reports, 13(1), 22432. https://doi.org/10.1038/s41598-023-50009-3 Baayen, R. H., Davidson, D. J. &amp; Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390–412. https://doi.org/10.1016/j.jml.2007.12.005 Bates, D., Mächler, M., Bolker, B. &amp; Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1), 1–48. https://doi.org/10.18637/jss.v067.i01 Gelman, A., Hill, J. &amp; Vehtari, A. (2020). Regression and other stories. Cambridge University Press. Hedeker, D. R. &amp; Gibbons, R. D. (2006). Longitudinal data analysis. John Wiley. Lowry, R. (2000). Concepts and applications of inferential statistics. http://vassarstats.net/textbook/ Vasishth, S., Schad, D., Bürki, A. &amp; Kliegl, R. (2022). Linear mixed models in linguistics and psychology: A comprehensive introduction. https://vasishth.github.io/Freq_CogSci Wickelmaier, F. (2022). Simulating the power of statistical tests: A collection of R examples. ArXiv. https://arxiv.org/abs/2110.09836 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
