# Simple and multiple regression

This chapter is meant to give a short introduction to regression analysis. It is
mostly meant to reintroduce some basic concepts, introduce notation, and get
everybody on the same page.

## Slides

```{=html}
<object data="slides/01_regression/01_regression.pdf" type="application/pdf" width="100%" height="500px">
  <p>
    Unable to display PDF file. <a href="slides/01_regression/01_regression.pdf">Download</a> instead.
  </p>
</object>
```


## Example to illustrate assumptions

```{r example}

data(anscombe)

lm1 <- lm(y1 ~ x1, anscombe)
lm2 <- lm(y2 ~ x2, anscombe)
lm3 <- lm(y3 ~ x3, anscombe)
lm4 <- lm(y4 ~ x4, anscombe)

# Compare estimates
rbind(coef(lm1), coef(lm2), coef(lm3), coef(lm4))

# Plot data
par(mfrow = c(2,2), mai = c(.6, .6, .1, .1), mgp = c(2.4, 1, 0))
plot(y1 ~ x1, anscombe, pch = 16, col = "blue")
abline(lm1, lwd = 2)
plot(y2 ~ x2, anscombe, pch = 16, col = "blue")
abline(lm2, lwd = 2)
plot(y3 ~ x3, anscombe, pch = 16, col = "blue")
abline(lm3, lwd = 2)
plot(y4 ~ x4, anscombe, pch = 16, col = "blue")
abline(lm4, lwd = 2)

# Look at assumption violations
par(mfrow = c(2,2))
plot(lm1)
plot(lm2)
plot(lm3)
plot(lm4)

# Reshape data frame to long format
dat <- reshape(anscombe,
               direction = "long",
               varying   = list(paste0("x", 1:4), paste0("y", 1:4)),
               timevar   = "reg",
               v.names   = c("x", "y"))[, -4]   # drop id

lattice::xyplot(y ~ x | as.factor(reg), dat, pch = 16, type = c("p", "r"))
lattice::xyplot(y ~ x | as.factor(reg), dat, pch = 16, type = c("p", "smooth"))
```

## Exercises

### Exercise 1 {-}

Simulate a data set based on a simple regression model with

\begin{align*}
  \beta_0 & = 0.2\\
  \beta_1 & = 0.3\\
  \sigma & = 0.5\\
  x & \in [1, 20]~\text{in steps of 1}
\end{align*}

What functions in R do we need?

```{r ex01.1, results = "hide", class.source = "fold-hide", fig.show = "hide"}
x <- 1:20
n <- length(x)
a <- 0.2
b <- 0.3
sigma <- 0.5
y <- 0.2 + 0.3*x + rnorm(n, sd = sigma)

dat <- data.frame(x, y)

lm1 <- lm(y ~ x, dat)
summary(lm1)

mean(resid(lm1))
sd(resid(lm1))
hist(resid(lm1), breaks = 15)

# plot data
plot(y ~ x, dat)
abline(lm1)
```

### Exercise 2 {-}

* Simulate data with the parameters from Exercise 1

* Do not assume that we have one subject per value for $x$, but more than one
  subject

* Simulate data for $n = 40$ and $n = 100$

  Hint: Use `sample(x, n, replace = TRUE)`

* Re-cover your parameters as done on slide 11

* What happens to your standard errors?


```{r ex01.2, results = "hide", class.source = "fold-hide", fig.show = "hide"}
n <- 100 # 40
x0 <- 1:20
x <- sample(x0, n, replace = TRUE)
a <- 0.2
b <- 0.3
sigma <- 0.5
y <- 0.2 + 0.3*x + rnorm(n, sd = sigma)

dat <- data.frame(x, y)

pars <- replicate(2000, {
  ysim <- 0.2 + 0.3 * x + rnorm(n, sd = sigma)
  lm1 <- lm(ysim ~ x, data = dat)
  c(coef(lm1), sigma(lm1))
})

rowMeans(pars)
# standard errors
apply(pars, 1, sd)

hist(pars[1, ])
hist(pars[2, ])
hist(pars[3, ])

plot(y ~ jitter(x), dat)
```

### Exercise 3 {-}

* Create two vectors $x$ and $y$ with 100 observations each and $X \sim N(1,1)$
  and $Y \sim N(2,1)$.

* Create a data frame with variables `id`, `group` and `score`. $x$ and $y$ are
  your score values.

* Conduct a $t$ test assuming that $X$ and $Y$ are independent having the same
  variances.

* Then use the function `aov()` to compute an analysis of variance for these
  data.

* Use then function `lm()` for a linear regression with predictor `group` and
  dependent variable `score`.

* Compare your results.

```{r ex01.3, results = "hide", class.source = "fold-hide"}
x   <- rnorm(100, mean = 1)
y   <- rnorm(100, mean = 2)
dat <- data.frame(id    = 1:200,
                  group = rep(c("x","y"), each = 100),
                  score = c(x, y))

rm(x,y)

t1   <- t.test(score ~ group, data = dat, var.equal = TRUE)
lm1  <- lm(score ~ group, data = dat)
aov1 <- aov(score ~ group, data = dat)
(stat <- list(
    coef = matrix(c(t1$estimate, lm1$coef, aov1$coef),
                  nrow = 2,
                  ncol = 3,
                  dimnames = list(NULL, c("ttest", "lm", "aov"))),
    statistics = matrix(c(t = t1$statistic^2,
                          Flm = summary(lm1)$fstatistic[1],
                          Faov = unlist(summary(aov1))[7]),
                        nrow = 1,
                        ncol = 3,
                        dimnames = list(NULL, c("t","Flm","Faov"))))
)
```

### Exercise 4 {-}

* The data set `cars` contains speed and stopping distances of 50 cars

* Estimate the regression model
  \[
    dist_i = \beta_0 + \beta_1 speed_i + \varepsilon_i
  \]

  - How much variance of the stopping distances is explained by speed?

  - Look at the residuals of the model. Are there any systematic deviances?
  
  - Now estimate the model
  \[
    dist_i = \beta_0 + \beta_1 speed_i + \beta_2 speed^2_i + \varepsilon_i
  \]

  Hint: Use `I(speed^2)` in the model formula
 
* Which model fits the data better?

```{r ex01.4, results = "hide", class.source = "fold-hide", fig.show = "hide"}
data(cars)

lm1 <- lm(dist ~ speed, data = cars)
summary(lm1)

hist(resid(lm1))

par(mfrow = c(2,2))
plot(lm1)

lm2 <- lm(dist ~ speed + I(speed^2), data = cars)
anova(lm1, lm2)
```

