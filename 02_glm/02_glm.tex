\documentclass[aspectratio=169]{beamer}
% \usepackage{pgfpages}
% \pgfpagesuselayout{4 on 1}[a4paper,landscape,border shrink=5mm]
\usepackage{tikz}
\usetikzlibrary{shapes, backgrounds, arrows, positioning}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage[utf8,latin1]{inputenc}
\usepackage[natbibapa]{apacite}
\makeatletter \def\newblock{\beamer@newblock} \makeatother  

\beamertemplatenavigationsymbolsempty
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{section in toc}[circle]
\mode<beamer>{\setbeamercolor{math text displayed}{fg=iwmgrau}}
\setbeamercolor{block body}{bg=iwmorange!50!white}
\setbeamercolor{block title}{fg=white, bg=iwmorange}

\definecolor{iwmorange}{RGB}{255,105,0}
\definecolor{iwmgrau}{RGB}{67,79,79}
\setbeamercolor{title}{fg=iwmorange}
\setbeamercolor{frametitle}{fg=iwmorange}
\setbeamercolor{structure}{fg=iwmorange}
\setbeamercolor{normal text}{fg=iwmgrau}
\setbeamercolor{author}{fg=iwmgrau}
\setbeamercolor{date}{fg=iwmgrau}
\color{white}

\title{Multiple regression and generalized linear models (GLM)}
\author{Nora Wickelmaier%\footnote{These slides are a modified version of slides created by \url{https://osf.io/ /}. }
}
%\institute{\includegraphics[scale=.15]{figures/ut_logo}}
\date{November 7, 2022}
%\date{Last modified: \today}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\gvect}[1]{\boldsymbol{#1}}
\newcommand{\gmat}[1]{\boldsymbol{#1}}

\lstset{language=R,%
  literate={Ü}{{\"U}}1
           {ü}{{\"u}}1,
  %backgroundcolor=\color{iwmgrau!80!white},
  basicstyle=\ttfamily\color{iwmorange},
  frame=single,
  commentstyle=\slshape\color{black},
  keywordstyle=\bfseries\color{white},
  identifierstyle=\color{white},
  stringstyle=\color{green!85!black},
  numbers=none,%left,numberstyle=\tiny,
  basewidth={.5em, .4em},
  showstringspaces=false,
  emphstyle=\color{red!50!white}}

\lstdefinestyle{plain}{language=R,
  frame=none,
  basicstyle=\ttfamily\color{iwmorange},
  commentstyle=\slshape\color{iwmgrau},
  keywordstyle=\bfseries\color{iwmgrau},
  identifierstyle=\color{iwmgrau},
  stringstyle=\color{iwmgrau},
  numbers=none,
  basewidth={.5em, .4em},
  showstringspaces=false}

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\AtBeginSection[]{
  \frame{
    \tableofcontents[sectionstyle=show/hide, subsectionstyle=show/show/hide]}}

\setbeamertemplate{headline}{
 \begin{beamercolorbox}{section in head}
   \vskip5pt\insertsectionnavigationhorizontal{\paperwidth}{}{}\vskip2pt
 \end{beamercolorbox}
}

\setbeamertemplate{footline}{\vskip-2pt\hfill\insertframenumber$\;$\vskip2pt}

\begin{document}

\begin{frame}{}
\thispagestyle{empty}
\titlepage
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\begin{frame}{}
  \begin{block}{Exercise}
    \begin{itemize}
      \item Create two vectors $x$ and $y$ with 100 observations each and
        $X \sim N(1,1)$ and $Y \sim N(2,1)$.
      \item Create a data frame with variables \texttt{id}, \texttt{group}
        and \texttt{score}. $x$ and $y$ are your score values.
      \item Conduct a $t$ test assuming that $X$ and $Y$ are independent 
        having the same variances.
      \item Then use the function \texttt{aov()} to compute an analysis of
        variance for these data.
      \item Use then function \texttt{lm()} for a linear regression with
        predictor \texttt{group} and dependent variable \texttt{score}.
      \item Compare your results.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Extending simple linear regression}
  \begin{tabular}{ll}
    Additional predictors &
      $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots +
      \varepsilon$\\
      & \\
    Nonlinear models &
      $\log y = \beta_0 + \beta_1 \log x + \varepsilon$\\
      & \\
    Nonadditive models &
      $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3
      x_1 x_2 + \varepsilon$\\
      & \\
    Generalized linear models &
      $g(E(y)) = \beta_0 + \beta_1 x$\\
      & \\
    Mixed-effects models &
      $y = \beta_0 + \beta_1 x_1 + \beta_2 time + 
      \upsilon_0 + \upsilon_1 time + \varepsilon$\\
      \dots & \\
  \end{tabular}
\end{frame}

\section{Multiple linear regression}

\begin{frame}{Multiple linear regression}
  \begin{itemize}
    \item Empirical observations consist of tuples for each observation
      unit
\[
  (y_i, x_{i1}, \ldots, x_{ip}) ~~\text{with}~~ i = 1, \ldots, n
\]
and we get the stochastical model
\begin{align*}
  y_i & = \beta_0 + \beta_1 \cdot x_{i1} + \ldots + \beta_p \cdot x_{ip} +
        \varepsilon_i \\
  \varepsilon_i & \sim N (0, \sigma^2)~\text{i.i.d.}
\end{align*}
which transfers to
\[
  y_i \sim N (\mu_i, \sigma^2) ~~\text{with}~~
  \mu_i = \beta_0 + \beta_1 \cdot x_{i1} + \ldots + \beta_p \cdot x_{ip}
\]\vspace{-.7cm}
\item The criterion variable $y$ is always a metric variable, whereas the
  predictor variables $x_1, \ldots, x_p$ can be either metric or
      categorical variables, or both
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example: Multiple linear regression}
  \begin{itemize}
    \item We are fitting the following model
\[
y_{ij} = \beta_0 + \beta_1 \cdot x_i + \beta_2 \cdot z_j + \varepsilon_{ij}
\]
with $i = 1 \ldots N$ and $j = 1,2$ for two groups
\item This means that we have one dummy variable for $z$ which takes the
  values 0 and 1
\item Hence, we get the two models
\begin{align*}
y_{i1} & = \beta_0 + \beta_1 \cdot x_i + \beta_2\cdot 0 + \varepsilon_{ij}
= \beta_0 + \beta_1 \cdot x_i + \varepsilon_{ij}\\
y_{i2} & = \beta_0 + \beta_1 \cdot x_i + \beta_2\cdot 1 + \varepsilon_{ij}
= (\beta_0 + \beta_2) + \beta_1 \cdot x_i + \varepsilon_{ij}
\end{align*}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Example: Multiple linear regression}
\begin{center}
\includegraphics[scale=.8]{figures/ancova2}
\end{center}
\end{frame}

{\setbeamercolor{background canvas}{bg=iwmgrau!80!white}

\begin{frame}[fragile]{Example: Multiple linear regression}
\begin{lstlisting}
dat <- data.frame(
  x = c(18,23,25,35,65,54,34,56,72,19,23,42,18,39,37),
  y = c(213,197,198,191,167,180,185,172,153,199,193,
        174,198,183,178),
  z = rep(c("treatment", "control"), c(7, 8))
)

aggregate(y ~ z, dat, mean)
\end{lstlisting}
\end{frame}

}


\begin{frame}[fragile]{Example: Multiple linear regression}
  \begin{itemize}
    \item We can now use the parameters to calculate adjusted means for the
      two groups
    \item The observed means are $\bar x_{treat} = 190.14$ and $\bar
      x_{contr} = 181.25$
    \item The adjusted means correspond to
\begin{align*}
\bar x_{contr} & = \beta_0\\
\bar x_{treat} & = \beta_0 + \beta_2
\end{align*}
These are the means for a value of $x = 0$ which should have a meaningful
interpretation
\item Hence, it might be indicated to center $x$
  \end{itemize}
\end{frame}

{\setbeamercolor{background canvas}{bg=iwmgrau!80!white}

\begin{frame}[fragile]{Example: Multiple linear regression}
\begin{lstlisting}
dat$xc <- dat$x - mean(dat$x)

lm2 <- lm(y ~ xc + z, dat)
summary(lm2)

# adjusted means
coef(lm2)[1]
coef(lm2)[1] + coef(lm2)[3]
\end{lstlisting}
\end{frame}

}

\begin{frame}[fragile]{}
  \begin{block}{Exercise}
    \begin{itemize}
      \item The data set \texttt{cars} contains speed and stopping
        distances of 50 cars
      \item Estimate the regression model
\[
  dist_i = \beta_0 + \beta_1 speed_i + \varepsilon_i
\]\vspace{-.8cm}
      \item How much variance of the stopping distances is explained by
      speed? \item Look at the residuals of the model. Are there any
        systematic deviances?
      \item Now estimate the model
\[
  dist_i = \beta_0 + \beta_1 speed_i + \beta_2 speed^2_i + \varepsilon_i
\]
        Hint: Use \verb+I(speed^2)+ in the model formula in \texttt{R}
      \item Which model fits the data better?
    \end{itemize}
  \end{block}
\end{frame}

\section{Generalized linear model}

\begin{frame}[allowframebreaks]{Intuition}
  \begin{itemize}
    \item In linear regerssion, a constant change in a predictor leads to a
      constant change in the response variable\\
        $\to$ implies that response variable can vary indefinitely in both
          directions (or only varies by a relative small amount)
    \item Generalized linear models allow for response variables that have
      \textbf{arbitrary distributions} (rather than simply normal
      distributions), and for an arbitrary function of the response
      variable (the link function) to vary linearly with the predictors
      (rather than assuming that the response itself must vary linearly)
      \framebreak
    \item Example 1: Model that predicts probability of making yes/no
      choice
      \begin{itemize}
        \item A model that predicts the likelihood of a given person going
          to the beach as a function of temperature
        \item A reasonable model might predict, for example, that a change
          in 10 degrees makes a person two times more or less likely to go
          to the beach
        \item That means th odds are doubling: from 2:1 odds to 4:1 odds
          and so on
      \end{itemize}
    \item Example 2: Model that predicts a certain count
      \begin{itemize}
        \item A realistic model would predict a constant rate of
          increased beach attendance (e.g. an increase of 10 degrees leads
          to a doubling in beach attendance, and a drop of 10 degrees leads
          to a halving in attendance
        \item This prediction would be independent of the size of the beach
      \end{itemize}
  \end{itemize}
  \vspace{.5cm}
{\footnotesize\url{https://en.wikipedia.org/wiki/Generalized_linear_model}}
\end{frame}

\begin{frame}[fragile]{Generalized linear models}
  \begin{itemize}
    \item A generalized linear model is defined by
\[
  g(E(y)) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k,
\]
where $g()$ is the link function that links the mean to the linear predictor.
The response $y$ is assumed to be independent and to follow a distribution
from the exponential family

\item In R, a GLM is fitted by

  \begin{lstlisting}[style=plain]
  glm(y ~ x1 + x2 + ... + xk, family(link), data)
\end{lstlisting}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Families}
  \begin{itemize}
    \item Each response distribution admits a variety of link functions to
      connect the mean with the linear predictor:

  \begin{lstlisting}[style=plain]
## Family name       Link functions
   binomial          logit, probit, log, cloglog
   gaussian          identity, log, inverse
   Gamma             identity, inverse, log
   inverse.gaussian  1/mu^2, identity, inverse, log
   poisson           log, identity, sqrt

   quasi             logit, probit, cloglog, identity,
                     inverse, log, 1/mu^2, sqrt
\end{lstlisting}
\item A GLM is a specific combination of a response distribution, a link
  function, and a linear predictor
  \end{itemize}
\end{frame}

\subsection{Logistic regression}

\begin{frame}{Binomial regression}
  \begin{itemize}
    \item Logit or probit models are special cases of GLMs for binomial
      response variables
    \item Artificial example: congenital eye disease
  \end{itemize}
\begin{columns}[c]
\begin{column}{6cm}
  \includegraphics[scale=.7]{figures/glm}
\end{column}
\begin{column}{5cm}
Logit model
\[
  \log\frac{p}{1 - p} = \beta_0 + \beta_1 AGE
\]
Probit model
\[
  \Phi^{-1}(p) = \beta_0 + \beta_1 AGE
\]
\end{column}
\end{columns}
\end{frame}

{\setbeamercolor{background canvas}{bg=iwmgrau!80!white}

\begin{frame}[fragile]{Fitting binomial regression models}
\begin{lstlisting}
dat <- data.frame(x = c(20,35,45,55,70), 
                  n = rep(50,5),
                  y = c(6,17,26,37,44))

glml <- glm(cbind(y, n - y) ~ x, binomial, dat)
glmp <- glm(cbind(y, n - y) ~ x, binomial(probit), dat)

# Parameter estimates
summary(glml)

# Interpretation as odds ratio
exp(coef(glml))
# --> Odds of going blind are increased by a factor
# of 1.08 when age increases by one year
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Goodness of fit and predictions}
\begin{lstlisting}
# Compare to saturated model
glms <- glm(cbind(y, n - y) ~ factor(x), binomial, dat)

# Likelihood ratio test
anova(glml, glms, test="Chisq")

# Predictions based on new observations
# (see ?predict.glm)
newx <- 0:100
predict(glml, data.frame(x=newx), type="response")
\end{lstlisting}
\end{frame}

}

\begin{frame}[fragile]{}
  \begin{block}{Exercise}
    \begin{itemize}
\item In a psychophysical experiment two LEDs are presented to a
  subject: a standard with 40\,cd/m$^2$ and a comparison with varying
        intensities
    \item The subject is supposed to say which stimulus is
      brighter; each comparison is presented 40 times
        \vspace{.2cm}
\begin{center}
\begin{tabular}{l|rrrrrrr}
x (cd/m$^2$)  & 37 & 38 & 39 & 40 & 41 & 42 & 43 \\ \hline
y (positiv)   &  2 &  3 & 10 & 25 & 34 & 36 & 39
\end{tabular}
\end{center}
        \vspace{.2cm}
\item Estimate parameters $c$ and $a$ of the logistic psychometric function
\[
  p_{pos} = \frac{1}{1 +
    \exp(-\frac{\displaystyle x - c}{\displaystyle a})}
\]
using \texttt{glm()} with $logit(p_{pos}) = \beta_0 + \beta_1x$ where 
        $a = 1/\beta_1$ and $c = -\beta_0/\beta_1$.
    \end{itemize}
  \end{block}
\end{frame}

%\begin{frame}[fragile]{}
%  \begin{block}{Homework}
%    \begin{itemize}
%      \item Calculate the intensity $x$ for which $p_{pos} = 0.5$ (Point of
%        Subjective Equality, PSE)
%      \item Create a plot for the probability to give a positive answer
%        depending on the intensity of the comparison
%      \item Use \texttt{predict()} to obtain the predicted values and add
%        the logistic psychometric function to the plot
%      \item Use \texttt{abline()} to add parameter $c$ to the plot
%    \end{itemize}
%  \end{block}
%\end{frame}

\subsection{Poisson regression}

\begin{frame}{Poisson distribution}
\begin{itemize}
  \item The Poisson distribution is popular for modelling the number of
    times an event occurs in an interval of time or space
\[
  P(k~\text{events in an interval}) = \exp(-\lambda)\frac{\lambda^k}{k!}
\]
where $k$ is the number of events in a certain interval and $\lambda$ is
the average number of events per interval
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Poisson distribution}
  Poisson distribution with probability of events for $\lambda = 2.5$\\[2ex]

\begin{columns}[c]
\begin{column}{6cm}
  \includegraphics[scale=.7]{figures/pois_dist}
\end{column}
\begin{column}{5cm}
  \begin{lstlisting}[style=plain]
x <- 0:8
px <- dpois(x, lambda=2.5)
plot(x, px, type="h")
\end{lstlisting}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Poisson regression}
  
Poisson regression is used to model count variables\footnote{Simulated dataset from
  \url{https://stats.idre.ucla.edu/stat/data/poisson_sim.csv}}.\\[2ex]
\begin{columns}[c]
\begin{column}{6cm}
  \includegraphics[scale=.7]{figures/pois_example}
\end{column}
\begin{column}{5cm}
  \begin{itemize}
    \item Number of awards earned by students at one high school
    \item Type of program in which student was enrolled (vocational,
    general, or academic)
    \item Score on students' final exam in math
  \end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Poisson regression}
  \begin{itemize}
    \item We estimate a poisson regression using a generalized linear model
      with \texttt{family = poisson} and link function \texttt{log}
  \begin{align*}
    g(E(y)) & = \beta_0 + \beta_1 x_1 + \beta_2 x_2\\
    \log(\mu) & = \beta_0 + \beta_1 prog + \beta_2 math
  \end{align*}
  with $y_i \sim \text{Poisson}(\lambda)$
  \end{itemize}
% % latex table generated in R 3.5.0 by xtable 1.8-2 package
% % Wed Jul 04 13:11:30 2018
% \begin{table}[ht]
% \centering\small
% \begin{tabular}{lrrrr}
%   \hline
%  & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
%   \hline
% (Intercept) & -5.2471 & 0.6585 & -7.97 & 0.0000 \\ 
%   progAcademic & 1.0839 & 0.3583 & 3.03 & 0.0025 \\ 
%   progVocational & 0.3698 & 0.4411 & 0.84 & 0.4018 \\ 
%   math & 0.0702 & 0.0106 & 6.62 & 0.0000 \\ 
%    \hline
% \end{tabular}
% \end{table}
\end{frame}

{\setbeamercolor{background canvas}{bg=iwmgrau!80!white}

\begin{frame}[fragile]{Poisson regression}
  \begin{lstlisting}
# Read data
dat <- read.csv("poisson_sim.csv")

# Define factors
dat$prog <- factor(dat$prog, levels=1:3,
  labels=c("General", "Academic", "Vocational"))
dat$id <- factor(dat$id)

# Fit poisson regression
m1 <- glm(num_awards ~ prog + math, family="poisson", data=dat)
summary(m1)

# Evaluate goodness-of-fit
1 - pchisq(m1$deviance, m1$df.residual)
\end{lstlisting}
\end{frame}

% \begin{frame}[fragile]{Poisson regression}
%   \begin{lstlisting}
% plot(num_awards ~ math, dat, type="n", ylim=c(-.5,6.1),
%   xlab="Math grade", ylab="Number of awards")
% 
% points(jitter(num_awards, 1) ~ jitter(math, 1), 
%   dat[dat$prog == "General",], pch=16, col="red")
% points(jitter(num_awards, 1) ~ jitter(math, 1), 
%   dat[dat$prog == "Academic",], pch=16, col="blue")
% points(jitter(num_awards, 1) ~ jitter(math, 1), 
%   dat[dat$prog == "Vocational",], pch=16, col="green")
% lines(phat ~ math, dat[dat$prog == "General",], col="red")
% lines(phat ~ math, dat[dat$prog == "Academic",], col="blue")
% lines(phat ~ math, dat[dat$prog == "Vocational",], col="green")
% 
% legend("topleft", c("General", "Academic", "Vocational"),
%   col=c("red", "blue", "green"), lty=1, pch=16, bty="n")
% \end{lstlisting}
% \end{frame}

}


\begin{frame}{Poisson regression}
\begin{itemize}
  \item The results show that the model fits the data with $G^2(196) =
    189.45,~p=0.6182$
  \item The expected number of awards when in the academic program is
    $\exp(1.0839) = 2.96$ times the expected number for the general program
    when math grade is held constant
  \item The expected number of awards increases by a factor of
    $\exp(0.0702) = 1.07$ when math grade increases by one unit (and
    program is held constant)
\end{itemize}
\end{frame}

\begin{frame}{Predictions of the poisson regression}
\begin{columns}[c]
\begin{column}{6cm}
  \includegraphics[scale=.7]{figures/pois_pre}
\end{column}
\begin{column}{5cm}
  \begin{itemize}
    \item Number of awards earned by students at one high school
    \item Type of program in which student was enrolled (vocational,
    general, or academic)
    \item Score on students' final exam in math
  \end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Overdispersion}
  \begin{itemize}
    \item Overdispersion is the presence of greater variability in a data
      set than would be expected based on a given statistical model
    \item It means that the underlying distributional assumptions might be
      violated
    \item The binomial and the poisson distribution are both less flexible
      than, e.\,g., the normal distribution, since they only have one free
      parameter and, therefore, the variance cannot be adjusted
      independently of the mean
    \item We can include a so-called overdispersion parameter $\varphi$
      into both models
    \item For the poisson regression, instead of assuming $E(y) = Var(y) =
      \mu$, we model $Var(y) = \varphi\mu$
  \end{itemize}
\end{frame}

{\setbeamercolor{background canvas}{bg=iwmgrau!80!white}

\begin{frame}[fragile]{Overdispersion}
% In R this is done by changing the \texttt{family} argument to
% \texttt{quasibinomial} or \texttt{quasipoisson}
\begin{lstlisting}
# Fit poisson regression
m2 <- glm(num_awards ~ prog + math, 
  family="quasipoisson", dat)
summary(m2)

# --> Results show that estimated parameters are
# still the same, but standard errors are slightly
# higher
\end{lstlisting}
\end{frame}

}

\begin{frame}{Some things to consider}
  \begin{itemize}
    \item When using \texttt{family = "quasipoisson"} or \texttt{family =
      "quasibinomial"}, likelihood-ratio tests are not meaningful anymore
      (even though R will let you do them)
    \item Goodness-of-fit tests are not necessarily meaningful for
      \emph{continuous} predictors for poisson and binomial regression, so
      use with caution
      (see, e.\,g.,
      \url{http://thestatsgeek.com/2014/04/26/deviance-goodness-of-fit-test-for-poisson-regression/})
  \end{itemize}
\end{frame}

% \appendix
% %\begin{frame}[allowframebreaks]{References}
% \begin{frame}{References}
% \renewcommand{\bibfont}{\footnotesize}
% \bibliographystyle{apacite}
% \bibliography{../../../literature/nu}
% \vfill
% \end{frame}

\end{document}

