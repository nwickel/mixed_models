\documentclass{beamer}
% \usepackage{pgfpages}
% \pgfpagesuselayout{4 on 1}[a4paper,landscape,border shrink=5mm]
\usepackage{tikz}
\usetikzlibrary{shapes, backgrounds, arrows, positioning}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage[utf8,latin1]{inputenc}
\usepackage[natbibapa]{apacite}
\makeatletter \def\newblock{\beamer@newblock} \makeatother  

\beamertemplatenavigationsymbolsempty
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{section in toc}[circle]
\mode<beamer>{\setbeamercolor{math text displayed}{fg=iwmgrau}}
\setbeamercolor{block body}{bg=iwmorange!50!white}
\setbeamercolor{block title}{fg=white, bg=iwmorange}

\definecolor{iwmorange}{RGB}{255,105,0}
\definecolor{iwmgrau}{RGB}{67,79,79}
\setbeamercolor{title}{fg=iwmorange}
\setbeamercolor{frametitle}{fg=iwmorange}
\setbeamercolor{structure}{fg=iwmorange}
\setbeamercolor{normal text}{fg=iwmgrau}
\setbeamercolor{author}{fg=iwmgrau}
\setbeamercolor{date}{fg=iwmgrau}
\color{white}

\title{Generalized linear models (GLM)}
\author{Nora Umbach%\footnote{These slides are a modified version of slides created by \url{https://osf.io/ /}. }
}
%\institute{\includegraphics[scale=.15]{figures/ut_logo}}
\date{May 3, 2021}
%\date{Last modified: \today}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\gvect}[1]{\boldsymbol{#1}}
\newcommand{\gmat}[1]{\boldsymbol{#1}}

\lstset{language=R,%
  literate={Ü}{{\"U}}1
           {ü}{{\"u}}1,
  %backgroundcolor=\color{iwmgrau!80!white},
  basicstyle=\ttfamily\color{iwmorange},
  frame=single,
  commentstyle=\slshape\color{black},
  keywordstyle=\bfseries\color{white},
  identifierstyle=\color{white},
  stringstyle=\color{green!85!black},
  numbers=none,%left,numberstyle=\tiny,
  basewidth={.5em, .4em},
  showstringspaces=false,
  emphstyle=\color{red!50!white}}

\lstdefinestyle{plain}{language=R,
  frame=none,
  basicstyle=\ttfamily\color{iwmorange},
  commentstyle=\slshape\color{iwmgrau},
  keywordstyle=\bfseries\color{iwmgrau},
  identifierstyle=\color{iwmgrau},
  stringstyle=\color{iwmgrau},
  numbers=none,
  basewidth={.5em, .4em},
  showstringspaces=false}

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\AtBeginSection[]{
  \frame{
    \tableofcontents[sectionstyle=show/hide, subsectionstyle=show/show/hide]}}

\setbeamertemplate{headline}{
 \begin{beamercolorbox}{section in head}
   \vskip5pt\insertsectionnavigationhorizontal{\paperwidth}{}{}\vskip2pt
 \end{beamercolorbox}
}

\setbeamertemplate{footline}{\vskip-2pt\hfill\insertframenumber$\;$\vskip2pt}

\begin{document}

\begin{frame}{}
\thispagestyle{empty}
\titlepage
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\begin{frame}{}
  \begin{block}{Exercise}
    \begin{itemize}
      \item Create two vectors $x$ and $y$ with 100 observations each and
        $X \sim N(1,1)$ and $Y \sim N(2,1)$.
      \item Create a data frame with variables \texttt{id}, \texttt{group}
        and \texttt{score}. $x$ and $y$ are your score values.
      \item Conduct a $t$ test assuming that $X$ and $Y$ are independent 
        having the same variances.
      \item Then use the function \texttt{aov()} to compute an analysis of
        variance for these data.
      \item Use then function \texttt{lm()} for a linear regression with
        predictor \texttt{group} and dependent variable \texttt{score}.
      \item Compare your results.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Extending simple linear regression}
  \begin{tabular}{ll}
    Additional predictors &
      $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots +
      \varepsilon$\\
      & \\
    Nonlinear models &
      $\log y = \beta_0 + \beta_1 \log x + \varepsilon$\\
      & \\
    Nonadditive models &
      $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3
      x_1 x_2 + \varepsilon$\\
      & \\
    Generalized linear models &
      $g(E(y)) = \beta_0 + \beta_1 x$\\
      & \\
    Mixed-effects models &
      $y = \beta_0 + \beta_1 x_1 + \beta_2 time + 
      \upsilon_0 + \upsilon_1 time + \varepsilon$\\
      \dots & \\
  \end{tabular}
\end{frame}

\begin{frame}[fragile]{Generalized linear models}
  \begin{itemize}
    \item A generalized linear model is defined by
\[
  g(E(y)) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k,
\]
where $g()$ is the link function that links the mean to the linear predictor.
The response $y$ is assumed to be independent and to follow a distribution
from the exponential family

\item In R, a GLM is fitted by

  \begin{lstlisting}[style=plain]
  glm(y ~ x1 + x2 + ... + xk, family(link), data)
\end{lstlisting}
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Families}
  \begin{itemize}
    \item Each response distribution admits a variety of link functions to
      connect the mean with the linear predictor:

  \begin{lstlisting}[style=plain]
## Family name       Link functions
   binomial          logit, probit, log, cloglog
   gaussian          identity, log, inverse
   Gamma             identity, inverse, log
   inverse.gaussian  1/mu^2, identity, inverse, log
   poisson           log, identity, sqrt

   quasi             logit, probit, cloglog, identity,
                     inverse, log, 1/mu^2, sqrt
\end{lstlisting}
\item A GLM is a specific combination of a response distribution, a link
  function, and a linear predictor
  \end{itemize}
\end{frame}

\section{Logistic regression}

\begin{frame}{Binomial regression}
  \begin{itemize}
    \item Logit or probit models are special cases of GLMs for binomial
      response variables
    \item Artificial example: congenital eye disease
  \end{itemize}
\begin{columns}[c]
\begin{column}{6cm}
  \includegraphics[scale=.7]{figures/glm}
\end{column}
\begin{column}{5cm}
Logit model
\[
  \log\frac{p}{1 - p} = \beta_0 + \beta_1 AGE
\]
Probit model
\[
  \Phi^{-1}(p) = \beta_0 + \beta_1 AGE
\]
\end{column}
\end{columns}
\end{frame}

{\setbeamercolor{background canvas}{bg=iwmgrau!80!white}

\begin{frame}[fragile]{Fitting binomial regression models}
\begin{lstlisting}
dat <- data.frame(x = c(20,35,45,55,70), 
                  n = rep(50,5),
                  y = c(6,17,26,37,44))

glml <- glm(cbind(y, n - y) ~ x, binomial, dat)
glmp <- glm(cbind(y, n - y) ~ x, binomial(probit), dat)

# Parameter estimates
summary(glml)

# Interpretation as odds ratio
exp(coef(glml))
# --> Odds of going blind are increased by a factor
# of 1.08 when age increases by one year
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Goodness of fit and predictions}
\begin{lstlisting}
# Compare to saturated model
glms <- glm(cbind(y, n - y) ~ factor(x), binomial,
  dat)

# Likelihood ratio test
anova(glml, glms, test="Chisq")

# Predictions based on new observations
# (see ?predict.glm)
newx <- 0:100
predict(glml, data.frame(x=newx), type="response")
\end{lstlisting}
\end{frame}

}

\section{Poisson regression}

\begin{frame}{Poisson distribution}
\begin{itemize}
  \item The Poisson distribution is popular for modelling the number of
    times an event occurs in an interval of time or space
\[
  P(k~\text{events in an interval}) = \exp(-\lambda)\frac{\lambda^k}{k!}
\]
where $k$ is the number of events in a certain interval and $\lambda$ is
the average number of events per interval
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Poisson distribution}
  Poisson distribution with probability of events for $\lambda = 2.5$\\[2ex]

\begin{columns}[c]
\begin{column}{6cm}
  \includegraphics[scale=.7]{figures/pois_dist}
\end{column}
\begin{column}{5cm}
  \begin{lstlisting}[style=plain]
x <- 0:8
px <- dpois(x, lambda=2.5)
plot(x, px, type="h")
\end{lstlisting}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Poisson regression}
  
Poisson regression is used to model count variables\footnote{Simulated dataset from
  \url{https://stats.idre.ucla.edu/stat/data/poisson_sim.csv}}.\\[2ex]
\begin{columns}[c]
\begin{column}{6cm}
  \includegraphics[scale=.7]{figures/pois_example}
\end{column}
\begin{column}{5cm}
  \begin{itemize}
    \item Number of awards earned by students at one high school
    \item Type of program in which student was enrolled (vocational,
    general, or academic)
    \item Score on students' final exam in math
  \end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Poisson regression}
  \begin{itemize}
    \item We estimate a poisson regression using a generalized linear model
      with \texttt{family = poisson} and link function \texttt{log}
  \begin{align*}
    g(E(y)) & = \beta_0 + \beta_1 x_1 + \beta_2 x_2\\
    \log(\mu) & = \beta_0 + \beta_1 prog + \beta_2 math
  \end{align*}
  with $y_i \sim \text{Poisson}(\lambda)$
  \end{itemize}
% % latex table generated in R 3.5.0 by xtable 1.8-2 package
% % Wed Jul 04 13:11:30 2018
% \begin{table}[ht]
% \centering\small
% \begin{tabular}{lrrrr}
%   \hline
%  & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
%   \hline
% (Intercept) & -5.2471 & 0.6585 & -7.97 & 0.0000 \\ 
%   progAcademic & 1.0839 & 0.3583 & 3.03 & 0.0025 \\ 
%   progVocational & 0.3698 & 0.4411 & 0.84 & 0.4018 \\ 
%   math & 0.0702 & 0.0106 & 6.62 & 0.0000 \\ 
%    \hline
% \end{tabular}
% \end{table}
\end{frame}

{\setbeamercolor{background canvas}{bg=iwmgrau!80!white}

\begin{frame}[fragile]{Poisson regression}
  \begin{lstlisting}
# Read data
dat <- read.csv("poisson_sim.csv")

# Define factors
dat$prog <- factor(dat$prog, levels=1:3,
  labels=c("General", "Academic", "Vocational"))
dat$id <- factor(dat$id)

# Fit poisson regression
m1 <- glm(num_awards ~ prog + math, family="poisson",
  data=dat)
summary(m1)

# Evaluate goodness-of-fit
1 - pchisq(m1$deviance, m1$df.residual)
\end{lstlisting}
\end{frame}

% \begin{frame}[fragile]{Poisson regression}
%   \begin{lstlisting}
% plot(num_awards ~ math, dat, type="n", ylim=c(-.5,6.1),
%   xlab="Math grade", ylab="Number of awards")
% 
% points(jitter(num_awards, 1) ~ jitter(math, 1), 
%   dat[dat$prog == "General",], pch=16, col="red")
% points(jitter(num_awards, 1) ~ jitter(math, 1), 
%   dat[dat$prog == "Academic",], pch=16, col="blue")
% points(jitter(num_awards, 1) ~ jitter(math, 1), 
%   dat[dat$prog == "Vocational",], pch=16, col="green")
% lines(phat ~ math, dat[dat$prog == "General",], col="red")
% lines(phat ~ math, dat[dat$prog == "Academic",], col="blue")
% lines(phat ~ math, dat[dat$prog == "Vocational",], col="green")
% 
% legend("topleft", c("General", "Academic", "Vocational"),
%   col=c("red", "blue", "green"), lty=1, pch=16, bty="n")
% \end{lstlisting}
% \end{frame}

}


\begin{frame}{Poisson regression}
\begin{itemize}
  \item The results show that the model fits the data with $G^2(196) =
    189.45,~p=0.6182$
  \item The expected number of awards when in the academic program is
    $\exp(1.0839) = 2.96$ times the expected number for the general program
    when math grade is held constant
  \item The expected number of awards increases by a factor of
    $\exp(0.0702) = 1.07$ when math grade increases by one unit (and
    program is held constant)
\end{itemize}
\end{frame}

\begin{frame}{Predictions of the poisson regression}
\begin{columns}[c]
\begin{column}{6cm}
  \includegraphics[scale=.7]{figures/pois_pre}
\end{column}
\begin{column}{5cm}
  \begin{itemize}
    \item Number of awards earned by students at one high school
    \item Type of program in which student was enrolled (vocational,
    general, or academic)
    \item Score on students' final exam in math
  \end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Overdispersion}
  \begin{itemize}
    \item Overdispersion is the presence of greater variability in a data
      set than would be expected based on a given statistical model
    \item It means that the underlying distributional assumptions might be
      violated
    \item The binomial and the poisson distribution are both less flexible
      than, e.\,g., the normal distribution, since they only have one free
      parameter and, therefore, the variance cannot be adjusted
      independently of the mean
    \item We can include a so-called overdispersion parameter $\varphi$
      into both models
    \item For the poisson regression, instead of assuming $E(y) = Var(y) =
      \mu$, we model $Var(y) = \varphi\mu$
  \end{itemize}
\end{frame}

{\setbeamercolor{background canvas}{bg=iwmgrau!80!white}

\begin{frame}[fragile]{Overdispersion}
% In R this is done by changing the \texttt{family} argument to
% \texttt{quasibinomial} or \texttt{quasipoisson}
\begin{lstlisting}
# Fit poisson regression
m2 <- glm(num_awards ~ prog + math, 
  family="quasipoisson", dat)
summary(m2)

# --> Results show that estimated parameters are
# still the same, but standard errors are slightly
# higher
\end{lstlisting}
\end{frame}

}

\begin{frame}{Some things to consider}
  \begin{itemize}
    \item When using \texttt{family = "quasipoisson"} or \texttt{family =
      "quasibinomial"}, likelihood-ratio tests are not meaningful anymore
      (even though R will let you do them)
    \item Goodness-of-fit tests are not necessarily meaningful for
      \emph{continuous} predictors for poisson and binomial regression, so
      use with caution
      (see, e.\,g.,
      \url{http://thestatsgeek.com/2014/04/26/deviance-goodness-of-fit-test-for-poisson-regression/})
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{}
  \begin{block}{Exercise}
    \begin{itemize}
\item In a psychophysical experiment two LEDs are presented to a
  subject: a standard with 40\,cd/m$^2$ and a comparison with varying
        intensities
    \item The subject is supposed to say which stimulus is
      brighter; each comparison is presented 40 times
        \vspace{.2cm}
\begin{center}
\begin{tabular}{l|rrrrrrr}
x (cd/m$^2$)  & 37 & 38 & 39 & 40 & 41 & 42 & 43 \\ \hline
y (positiv)   &  2 &  3 & 10 & 25 & 34 & 36 & 39
\end{tabular}
\end{center}
        \vspace{.2cm}
\item Estimate parameters $c$ and $a$ of the logistic psychometric function
\[
  p_{pos} = \frac{1}{1 +
    \exp(-\frac{\displaystyle x - c}{\displaystyle a})}
\]
using \texttt{glm()} with $logit(p_{pos}) = \beta_0 + \beta_1x$ where 
        $a = 1/\beta_1$ and $c = -\beta_0/\beta_1$.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}[fragile]{}
  \begin{block}{Homework}
    \begin{itemize}
      \item Calculate the intensity $x$ for which $p_{pos} = 0.5$ (Point of
        Subjective Equality, PSE)
      \item Create a plot for the probability to give a positive answer
        depending on the intensity of the comparison
      \item Use \texttt{predict()} to obtain the predicted values and add
        the logistic psychometric function to the plot
      \item Use \texttt{abline()} to add parameter $c$ to the plot
    \end{itemize}
  \end{block}
\end{frame}

% \appendix
% %\begin{frame}[allowframebreaks]{References}
% \begin{frame}{References}
% \renewcommand{\bibfont}{\footnotesize}
% \bibliographystyle{apacite}
% \bibliography{../../../literature/nu}
% \vfill
% \end{frame}

\end{document}

